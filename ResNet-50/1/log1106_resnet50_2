[2021-11-05 11:19:33] args:Namespace(ablation_id=0, arch='resnet_50', batch_size=64, compress_rate='[0.]+[0.1,0.1,0.2]*1+[0.5,0.5,0.2]*2+[0.1,0.1,0.2]*1+[0.5,0.5,0.2]*3+[0.1,0.1,0.2]*1+[0.5,0.5,0.2]*5+[0.1,0.1,0.1]+[0.2,0.2,0.1]*2', data_dir='/home/featurize/data', dataset='ImageNet', epochs=30, from_scratch=True, gpu=0, input_size=224, job_dir='/home/featurize/work/CCCrank5', lr=0.001, lr_decay_step='5,15', momentum=0.9, num_workers=8, pretrained=False, resume='pruned_checkpoint/resnet_50_cov52.pt', save_id=11, start_cov=-1, weight_decay=0.0005)
[2021-11-05 11:19:34] loading checkpoint:pruned_checkpoint/resnet_50_cov52.pt
[2021-11-05 11:19:34] loading ranks form checkpoint...
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv0.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv1.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv2.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv3.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv3.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv4.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv5.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv6.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv7.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv8.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv9.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv10.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv11.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv12.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv12.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv13.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv14.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv15.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv16.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv17.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv18.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv19.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv20.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv21.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv22.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv23.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv24.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv24.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv25.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv26.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv27.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv28.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv29.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv30.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv31.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv32.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv33.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv34.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv35.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv36.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv37.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv38.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv39.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv40.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv41.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv42.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv42.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv43.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv44.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv45.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv46.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv47.npy
[2021-11-05 11:19:34] loading rank_conv/resnet_50/rank_conv48.npy
[2021-11-05 11:20:59] validate Loss: 1.0707 Acc@1: 73.40 Acc@5: 91.45 time: 85.1584
[2021-11-05 11:20:59] epoch 0 learning_rate 0.001 
[2021-11-05 11:26:55] epoch[0](2000/20000) Loss: 1.2341 Acc@1: 70.31 Acc@5: 88.30 time: 355.0676
[2021-11-05 11:32:47] epoch[0](4000/20000) Loss: 1.2536 Acc@1: 69.83 Acc@5: 88.09 time: 352.4831
[2021-11-05 11:38:39] epoch[0](6000/20000) Loss: 1.2625 Acc@1: 69.66 Acc@5: 88.00 time: 352.3189
[2021-11-05 11:44:32] epoch[0](8000/20000) Loss: 1.2661 Acc@1: 69.61 Acc@5: 87.98 time: 352.5647
[2021-11-05 11:50:24] epoch[0](10000/20000) Loss: 1.2698 Acc@1: 69.53 Acc@5: 87.95 time: 352.2918
[2021-11-05 11:56:17] epoch[0](12000/20000) Loss: 1.2727 Acc@1: 69.47 Acc@5: 87.92 time: 352.5200
[2021-11-05 12:02:09] epoch[0](14000/20000) Loss: 1.2769 Acc@1: 69.40 Acc@5: 87.87 time: 352.5369
[2021-11-05 12:08:02] epoch[0](16000/20000) Loss: 1.2803 Acc@1: 69.34 Acc@5: 87.83 time: 352.4021
[2021-11-05 12:13:54] epoch[0](18000/20000) Loss: 1.2837 Acc@1: 69.28 Acc@5: 87.79 time: 352.4923
[2021-11-05 12:19:47] epoch[0](20000/20000) Loss: 1.2868 Acc@1: 69.22 Acc@5: 87.75 time: 352.3812
[2021-11-05 12:19:50] train    Loss: 1.2868 Acc@1: 69.22 Acc@5: 87.75 time: 3530.4380
[2021-11-05 12:21:16] validate Loss: 1.1421 Acc@1: 71.43 Acc@5: 90.38 time: 85.8175
[2021-11-05 12:21:16] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-05 12:21:16] epoch 1 learning_rate 0.001 
[2021-11-05 12:27:11] epoch[1](2000/20000) Loss: 1.2758 Acc@1: 69.68 Acc@5: 88.00 time: 355.1530
[2021-11-05 12:33:04] epoch[1](4000/20000) Loss: 1.2802 Acc@1: 69.59 Acc@5: 87.93 time: 352.4948
[2021-11-05 12:38:56] epoch[1](6000/20000) Loss: 1.2882 Acc@1: 69.39 Acc@5: 87.84 time: 352.2984
[2021-11-05 12:44:49] epoch[1](8000/20000) Loss: 1.2939 Acc@1: 69.26 Acc@5: 87.79 time: 352.5571
[2021-11-05 12:50:41] epoch[1](10000/20000) Loss: 1.2987 Acc@1: 69.18 Acc@5: 87.72 time: 352.1975
[2021-11-05 12:56:33] epoch[1](12000/20000) Loss: 1.3013 Acc@1: 69.11 Acc@5: 87.70 time: 352.3772
[2021-11-05 13:02:25] epoch[1](14000/20000) Loss: 1.3035 Acc@1: 69.07 Acc@5: 87.68 time: 352.3603
[2021-11-05 13:08:18] epoch[1](16000/20000) Loss: 1.3075 Acc@1: 68.99 Acc@5: 87.64 time: 352.4371
[2021-11-05 13:14:10] epoch[1](18000/20000) Loss: 1.3097 Acc@1: 68.95 Acc@5: 87.61 time: 352.4058
[2021-11-05 13:20:03] epoch[1](20000/20000) Loss: 1.3135 Acc@1: 68.87 Acc@5: 87.56 time: 352.3075
[2021-11-05 13:20:06] train    Loss: 1.3135 Acc@1: 68.87 Acc@5: 87.55 time: 3529.9117
[2021-11-05 13:21:32] validate Loss: 1.1511 Acc@1: 71.34 Acc@5: 90.37 time: 86.1606
[2021-11-05 13:21:32] epoch 2 learning_rate 0.001 
[2021-11-05 13:27:27] epoch[2](2000/20000) Loss: 1.3076 Acc@1: 69.20 Acc@5: 87.77 time: 354.9289
[2021-11-05 13:33:19] epoch[2](4000/20000) Loss: 1.3113 Acc@1: 69.13 Acc@5: 87.74 time: 352.1953
[2021-11-05 13:39:12] epoch[2](6000/20000) Loss: 1.3160 Acc@1: 69.03 Acc@5: 87.68 time: 352.4003
[2021-11-05 13:45:04] epoch[2](8000/20000) Loss: 1.3213 Acc@1: 68.91 Acc@5: 87.60 time: 352.4576
[2021-11-05 13:50:57] epoch[2](10000/20000) Loss: 1.3254 Acc@1: 68.83 Acc@5: 87.55 time: 352.5020
[2021-11-05 13:56:49] epoch[2](12000/20000) Loss: 1.3305 Acc@1: 68.73 Acc@5: 87.47 time: 352.4372
[2021-11-05 14:02:41] epoch[2](14000/20000) Loss: 1.3333 Acc@1: 68.67 Acc@5: 87.44 time: 352.4255
[2021-11-05 14:08:34] epoch[2](16000/20000) Loss: 1.3381 Acc@1: 68.56 Acc@5: 87.37 time: 352.3417
[2021-11-05 14:14:26] epoch[2](18000/20000) Loss: 1.3415 Acc@1: 68.48 Acc@5: 87.33 time: 352.3857
[2021-11-05 14:20:19] epoch[2](20000/20000) Loss: 1.3460 Acc@1: 68.39 Acc@5: 87.27 time: 352.4486
[2021-11-05 14:20:22] train    Loss: 1.3460 Acc@1: 68.39 Acc@5: 87.27 time: 3529.8640
[2021-11-05 14:21:47] validate Loss: 1.1685 Acc@1: 70.86 Acc@5: 90.20 time: 85.4358
[2021-11-05 14:21:47] epoch 3 learning_rate 0.001 
[2021-11-05 14:27:42] epoch[3](2000/20000) Loss: 1.3457 Acc@1: 68.39 Acc@5: 87.36 time: 354.9082
[2021-11-05 14:33:35] epoch[3](4000/20000) Loss: 1.3491 Acc@1: 68.38 Acc@5: 87.39 time: 352.5976
[2021-11-05 14:39:27] epoch[3](6000/20000) Loss: 1.3563 Acc@1: 68.28 Acc@5: 87.24 time: 352.5024
[2021-11-05 14:45:20] epoch[3](8000/20000) Loss: 1.3612 Acc@1: 68.17 Acc@5: 87.17 time: 352.6206
[2021-11-05 14:51:13] epoch[3](10000/20000) Loss: 1.3655 Acc@1: 68.08 Acc@5: 87.12 time: 352.4938
[2021-11-05 14:57:05] epoch[3](12000/20000) Loss: 1.3713 Acc@1: 67.96 Acc@5: 87.05 time: 352.6280
[2021-11-05 15:02:58] epoch[3](14000/20000) Loss: 1.3742 Acc@1: 67.90 Acc@5: 87.03 time: 352.6123
[2021-11-05 15:08:50] epoch[3](16000/20000) Loss: 1.3785 Acc@1: 67.81 Acc@5: 86.97 time: 352.5219
[2021-11-05 15:14:43] epoch[3](18000/20000) Loss: 1.3831 Acc@1: 67.72 Acc@5: 86.91 time: 352.3480
[2021-11-05 15:20:35] epoch[3](20000/20000) Loss: 1.3857 Acc@1: 67.67 Acc@5: 86.88 time: 352.5607
[2021-11-05 15:20:39] train    Loss: 1.3858 Acc@1: 67.67 Acc@5: 86.88 time: 3531.1338
[2021-11-05 15:22:07] validate Loss: 1.1901 Acc@1: 70.35 Acc@5: 89.89 time: 88.3391
[2021-11-05 15:22:07] epoch 4 learning_rate 0.001 
[2021-11-05 15:28:02] epoch[4](2000/20000) Loss: 1.3774 Acc@1: 68.01 Acc@5: 87.04 time: 355.1280
[2021-11-05 15:33:54] epoch[4](4000/20000) Loss: 1.3878 Acc@1: 67.75 Acc@5: 86.92 time: 352.4197
[2021-11-05 15:39:47] epoch[4](6000/20000) Loss: 1.3974 Acc@1: 67.55 Acc@5: 86.77 time: 352.3228
[2021-11-05 15:45:39] epoch[4](8000/20000) Loss: 1.4025 Acc@1: 67.43 Acc@5: 86.72 time: 352.6054
[2021-11-05 15:51:32] epoch[4](10000/20000) Loss: 1.4083 Acc@1: 67.26 Acc@5: 86.64 time: 352.5774
[2021-11-05 15:57:24] epoch[4](12000/20000) Loss: 1.4151 Acc@1: 67.14 Acc@5: 86.53 time: 352.4710
[2021-11-05 16:03:17] epoch[4](14000/20000) Loss: 1.4193 Acc@1: 67.04 Acc@5: 86.48 time: 352.5002
[2021-11-05 16:09:09] epoch[4](16000/20000) Loss: 1.4233 Acc@1: 66.97 Acc@5: 86.43 time: 352.5038
[2021-11-05 16:15:02] epoch[4](18000/20000) Loss: 1.4275 Acc@1: 66.86 Acc@5: 86.37 time: 352.5305
[2021-11-05 16:20:54] epoch[4](20000/20000) Loss: 1.4311 Acc@1: 66.79 Acc@5: 86.33 time: 352.5158
[2021-11-05 16:20:58] train    Loss: 1.4311 Acc@1: 66.79 Acc@5: 86.33 time: 3530.9665
[2021-11-05 16:22:32] validate Loss: 1.2103 Acc@1: 69.96 Acc@5: 89.81 time: 94.0285
[2021-11-05 16:22:32] epoch 5 learning_rate 0.0001 
[2021-11-05 16:28:27] epoch[5](2000/20000) Loss: 1.3382 Acc@1: 68.98 Acc@5: 87.59 time: 355.3677
[2021-11-05 16:34:20] epoch[5](4000/20000) Loss: 1.3157 Acc@1: 69.46 Acc@5: 87.81 time: 352.6372
[2021-11-05 16:40:12] epoch[5](6000/20000) Loss: 1.2982 Acc@1: 69.82 Acc@5: 88.05 time: 352.3496
[2021-11-05 16:46:05] epoch[5](8000/20000) Loss: 1.2887 Acc@1: 70.01 Acc@5: 88.16 time: 352.6115
[2021-11-05 16:51:57] epoch[5](10000/20000) Loss: 1.2811 Acc@1: 70.19 Acc@5: 88.24 time: 352.2742
[2021-11-05 16:57:50] epoch[5](12000/20000) Loss: 1.2748 Acc@1: 70.36 Acc@5: 88.31 time: 352.5180
[2021-11-05 17:03:42] epoch[5](14000/20000) Loss: 1.2692 Acc@1: 70.48 Acc@5: 88.38 time: 352.1966
[2021-11-05 17:09:34] epoch[5](16000/20000) Loss: 1.2661 Acc@1: 70.54 Acc@5: 88.42 time: 352.3739
[2021-11-05 17:15:27] epoch[5](18000/20000) Loss: 1.2632 Acc@1: 70.60 Acc@5: 88.47 time: 352.2654
[2021-11-05 17:21:19] epoch[5](20000/20000) Loss: 1.2608 Acc@1: 70.65 Acc@5: 88.51 time: 352.4694
[2021-11-05 17:21:22] train    Loss: 1.2608 Acc@1: 70.65 Acc@5: 88.51 time: 3530.4653
[2021-11-05 17:22:57] validate Loss: 1.0566 Acc@1: 73.52 Acc@5: 91.59 time: 95.0504
[2021-11-05 17:22:58] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-05 17:22:58] epoch 6 learning_rate 0.0001 
[2021-11-05 17:28:53] epoch[6](2000/20000) Loss: 1.2200 Acc@1: 71.59 Acc@5: 89.01 time: 355.0670
[2021-11-05 17:34:45] epoch[6](4000/20000) Loss: 1.2207 Acc@1: 71.57 Acc@5: 89.01 time: 352.2013
[2021-11-05 17:40:37] epoch[6](6000/20000) Loss: 1.2190 Acc@1: 71.59 Acc@5: 89.03 time: 352.2913
[2021-11-05 17:46:30] epoch[6](8000/20000) Loss: 1.2193 Acc@1: 71.59 Acc@5: 89.03 time: 352.3423
[2021-11-05 17:52:22] epoch[6](10000/20000) Loss: 1.2186 Acc@1: 71.59 Acc@5: 89.02 time: 352.4382
[2021-11-05 17:58:15] epoch[6](12000/20000) Loss: 1.2177 Acc@1: 71.59 Acc@5: 89.04 time: 352.5375
[2021-11-05 18:04:07] epoch[6](14000/20000) Loss: 1.2163 Acc@1: 71.63 Acc@5: 89.06 time: 352.3346
[2021-11-05 18:09:59] epoch[6](16000/20000) Loss: 1.2161 Acc@1: 71.63 Acc@5: 89.07 time: 352.2955
[2021-11-05 18:15:52] epoch[6](18000/20000) Loss: 1.2158 Acc@1: 71.64 Acc@5: 89.07 time: 352.4351
[2021-11-05 18:21:44] epoch[6](20000/20000) Loss: 1.2152 Acc@1: 71.65 Acc@5: 89.08 time: 352.2142
[2021-11-05 18:21:47] train    Loss: 1.2152 Acc@1: 71.65 Acc@5: 89.08 time: 3529.5118
[2021-11-05 18:23:19] validate Loss: 1.0446 Acc@1: 73.59 Acc@5: 91.73 time: 91.9105
[2021-11-05 18:23:20] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-05 18:23:20] epoch 7 learning_rate 0.0001 
[2021-11-05 18:29:15] epoch[7](2000/20000) Loss: 1.2024 Acc@1: 72.01 Acc@5: 89.19 time: 355.0521
[2021-11-05 18:35:07] epoch[7](4000/20000) Loss: 1.2001 Acc@1: 72.00 Acc@5: 89.24 time: 352.1893
[2021-11-05 18:40:59] epoch[7](6000/20000) Loss: 1.2022 Acc@1: 71.96 Acc@5: 89.22 time: 352.4314
[2021-11-05 18:46:51] epoch[7](8000/20000) Loss: 1.2011 Acc@1: 72.01 Acc@5: 89.23 time: 352.1798
[2021-11-05 18:52:44] epoch[7](10000/20000) Loss: 1.2005 Acc@1: 72.01 Acc@5: 89.23 time: 352.3891
[2021-11-05 18:58:36] epoch[7](12000/20000) Loss: 1.2004 Acc@1: 71.99 Acc@5: 89.24 time: 352.2742
[2021-11-05 19:04:28] epoch[7](14000/20000) Loss: 1.1998 Acc@1: 71.99 Acc@5: 89.25 time: 352.3896
[2021-11-05 19:10:21] epoch[7](16000/20000) Loss: 1.1998 Acc@1: 72.02 Acc@5: 89.25 time: 352.4315
[2021-11-05 19:16:13] epoch[7](18000/20000) Loss: 1.2002 Acc@1: 72.00 Acc@5: 89.25 time: 352.4612
[2021-11-05 19:22:06] epoch[7](20000/20000) Loss: 1.1998 Acc@1: 72.01 Acc@5: 89.25 time: 352.4997
[2021-11-05 19:22:09] train    Loss: 1.1998 Acc@1: 72.01 Acc@5: 89.26 time: 3529.6727
[2021-11-05 19:23:42] validate Loss: 1.0368 Acc@1: 73.91 Acc@5: 91.79 time: 93.0778
[2021-11-05 19:23:43] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-05 19:23:43] epoch 8 learning_rate 0.0001 
[2021-11-05 19:29:38] epoch[8](2000/20000) Loss: 1.1868 Acc@1: 72.24 Acc@5: 89.44 time: 355.3382
[2021-11-05 19:35:30] epoch[8](4000/20000) Loss: 1.1886 Acc@1: 72.22 Acc@5: 89.44 time: 352.2423
[2021-11-05 19:41:23] epoch[8](6000/20000) Loss: 1.1887 Acc@1: 72.24 Acc@5: 89.43 time: 352.3642
[2021-11-05 19:47:15] epoch[8](8000/20000) Loss: 1.1893 Acc@1: 72.24 Acc@5: 89.42 time: 352.4416
[2021-11-05 19:53:08] epoch[8](10000/20000) Loss: 1.1884 Acc@1: 72.24 Acc@5: 89.44 time: 352.4833
[2021-11-05 19:59:00] epoch[8](12000/20000) Loss: 1.1894 Acc@1: 72.23 Acc@5: 89.41 time: 352.3565
[2021-11-05 20:04:52] epoch[8](14000/20000) Loss: 1.1891 Acc@1: 72.23 Acc@5: 89.41 time: 352.4194
[2021-11-05 20:10:45] epoch[8](16000/20000) Loss: 1.1884 Acc@1: 72.25 Acc@5: 89.43 time: 352.4502
[2021-11-05 20:16:37] epoch[8](18000/20000) Loss: 1.1884 Acc@1: 72.27 Acc@5: 89.41 time: 352.5106
[2021-11-05 20:22:30] epoch[8](20000/20000) Loss: 1.1886 Acc@1: 72.27 Acc@5: 89.41 time: 352.8268
[2021-11-05 20:22:34] train    Loss: 1.1885 Acc@1: 72.27 Acc@5: 89.41 time: 3530.8329
[2021-11-05 20:24:05] validate Loss: 1.0341 Acc@1: 73.92 Acc@5: 91.88 time: 91.6104
[2021-11-05 20:24:06] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-05 20:24:06] epoch 9 learning_rate 0.0001 
[2021-11-05 20:30:01] epoch[9](2000/20000) Loss: 1.1865 Acc@1: 72.31 Acc@5: 89.40 time: 355.6362
[2021-11-05 20:35:54] epoch[9](4000/20000) Loss: 1.1859 Acc@1: 72.38 Acc@5: 89.44 time: 352.7241
[2021-11-05 20:41:46] epoch[9](6000/20000) Loss: 1.1811 Acc@1: 72.47 Acc@5: 89.50 time: 352.3702
[2021-11-05 20:47:39] epoch[9](8000/20000) Loss: 1.1807 Acc@1: 72.47 Acc@5: 89.51 time: 352.5239
[2021-11-05 20:53:31] epoch[9](10000/20000) Loss: 1.1808 Acc@1: 72.48 Acc@5: 89.50 time: 352.3365
[2021-11-05 20:59:23] epoch[9](12000/20000) Loss: 1.1821 Acc@1: 72.45 Acc@5: 89.49 time: 352.2723
[2021-11-05 21:05:16] epoch[9](14000/20000) Loss: 1.1828 Acc@1: 72.44 Acc@5: 89.47 time: 352.5355
[2021-11-05 21:11:08] epoch[9](16000/20000) Loss: 1.1823 Acc@1: 72.44 Acc@5: 89.48 time: 352.2696
[2021-11-05 21:17:01] epoch[9](18000/20000) Loss: 1.1822 Acc@1: 72.45 Acc@5: 89.48 time: 352.3842
[2021-11-05 21:22:53] epoch[9](20000/20000) Loss: 1.1817 Acc@1: 72.46 Acc@5: 89.49 time: 352.5178
[2021-11-05 21:22:56] train    Loss: 1.1818 Acc@1: 72.46 Acc@5: 89.49 time: 3530.9239
[2021-11-05 21:24:28] validate Loss: 1.0312 Acc@1: 74.00 Acc@5: 91.89 time: 91.1249
[2021-11-05 21:24:28] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-05 21:24:28] epoch 10 learning_rate 0.0001 
[2021-11-05 21:30:23] epoch[10](2000/20000) Loss: 1.1639 Acc@1: 72.79 Acc@5: 89.67 time: 355.4912
[2021-11-05 21:36:16] epoch[10](4000/20000) Loss: 1.1679 Acc@1: 72.66 Acc@5: 89.66 time: 352.5505
[2021-11-05 21:42:09] epoch[10](6000/20000) Loss: 1.1684 Acc@1: 72.64 Acc@5: 89.67 time: 353.2426
[2021-11-05 21:48:02] epoch[10](8000/20000) Loss: 1.1698 Acc@1: 72.64 Acc@5: 89.65 time: 352.8144
[2021-11-05 21:53:54] epoch[10](10000/20000) Loss: 1.1695 Acc@1: 72.66 Acc@5: 89.65 time: 352.4208
[2021-11-05 21:59:48] epoch[10](12000/20000) Loss: 1.1699 Acc@1: 72.65 Acc@5: 89.64 time: 353.4091
[2021-11-05 22:05:41] epoch[10](14000/20000) Loss: 1.1702 Acc@1: 72.65 Acc@5: 89.64 time: 353.3190
[2021-11-05 22:11:34] epoch[10](16000/20000) Loss: 1.1710 Acc@1: 72.66 Acc@5: 89.63 time: 353.2857
[2021-11-05 22:17:28] epoch[10](18000/20000) Loss: 1.1722 Acc@1: 72.63 Acc@5: 89.61 time: 353.4003
[2021-11-05 22:23:21] epoch[10](20000/20000) Loss: 1.1727 Acc@1: 72.63 Acc@5: 89.61 time: 353.0086
[2021-11-05 22:23:24] train    Loss: 1.1727 Acc@1: 72.63 Acc@5: 89.61 time: 3536.3885
[2021-11-05 22:25:07] validate Loss: 1.0253 Acc@1: 74.09 Acc@5: 92.07 time: 102.3380
[2021-11-05 22:25:07] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-05 22:25:07] epoch 11 learning_rate 0.0001 
[2021-11-05 22:31:03] epoch[11](2000/20000) Loss: 1.1700 Acc@1: 72.70 Acc@5: 89.66 time: 356.2284
[2021-11-05 22:36:57] epoch[11](4000/20000) Loss: 1.1677 Acc@1: 72.68 Acc@5: 89.66 time: 353.3161
[2021-11-05 22:42:50] epoch[11](6000/20000) Loss: 1.1687 Acc@1: 72.75 Acc@5: 89.63 time: 353.2977
[2021-11-05 22:48:43] epoch[11](8000/20000) Loss: 1.1685 Acc@1: 72.75 Acc@5: 89.66 time: 353.0656
[2021-11-05 22:54:36] epoch[11](10000/20000) Loss: 1.1679 Acc@1: 72.73 Acc@5: 89.68 time: 353.2084
[2021-11-05 23:00:29] epoch[11](12000/20000) Loss: 1.1682 Acc@1: 72.74 Acc@5: 89.66 time: 353.2342
[2021-11-05 23:06:23] epoch[11](14000/20000) Loss: 1.1681 Acc@1: 72.72 Acc@5: 89.68 time: 353.2035
[2021-11-05 23:12:16] epoch[11](16000/20000) Loss: 1.1680 Acc@1: 72.72 Acc@5: 89.66 time: 353.3025
[2021-11-05 23:18:09] epoch[11](18000/20000) Loss: 1.1681 Acc@1: 72.72 Acc@5: 89.66 time: 353.2035
[2021-11-05 23:24:02] epoch[11](20000/20000) Loss: 1.1681 Acc@1: 72.73 Acc@5: 89.67 time: 353.1489
[2021-11-05 23:24:06] train    Loss: 1.1681 Acc@1: 72.73 Acc@5: 89.67 time: 3538.6176
[2021-11-05 23:25:46] validate Loss: 1.0247 Acc@1: 74.30 Acc@5: 91.95 time: 100.1674
[2021-11-05 23:25:46] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-05 23:25:46] epoch 12 learning_rate 0.0001 
[2021-11-05 23:31:42] epoch[12](2000/20000) Loss: 1.1686 Acc@1: 72.79 Acc@5: 89.62 time: 356.0653
[2021-11-05 23:37:36] epoch[12](4000/20000) Loss: 1.1661 Acc@1: 72.85 Acc@5: 89.68 time: 353.3703
[2021-11-05 23:43:29] epoch[12](6000/20000) Loss: 1.1677 Acc@1: 72.77 Acc@5: 89.67 time: 353.2133
[2021-11-05 23:49:22] epoch[12](8000/20000) Loss: 1.1661 Acc@1: 72.79 Acc@5: 89.68 time: 353.3136
[2021-11-05 23:55:15] epoch[12](10000/20000) Loss: 1.1650 Acc@1: 72.79 Acc@5: 89.70 time: 353.2674
[2021-11-06 00:01:09] epoch[12](12000/20000) Loss: 1.1652 Acc@1: 72.78 Acc@5: 89.71 time: 353.5231
[2021-11-06 00:07:02] epoch[12](14000/20000) Loss: 1.1648 Acc@1: 72.79 Acc@5: 89.71 time: 353.1986
[2021-11-06 00:12:56] epoch[12](16000/20000) Loss: 1.1657 Acc@1: 72.79 Acc@5: 89.69 time: 353.4691
[2021-11-06 00:18:49] epoch[12](18000/20000) Loss: 1.1650 Acc@1: 72.82 Acc@5: 89.71 time: 353.3374
[2021-11-06 00:24:42] epoch[12](20000/20000) Loss: 1.1661 Acc@1: 72.80 Acc@5: 89.68 time: 353.3395
[2021-11-06 00:24:46] train    Loss: 1.1660 Acc@1: 72.80 Acc@5: 89.68 time: 3539.4922
[2021-11-06 00:26:26] validate Loss: 1.0221 Acc@1: 74.37 Acc@5: 92.02 time: 100.1767
[2021-11-06 00:26:26] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-06 00:26:26] epoch 13 learning_rate 0.0001 
[2021-11-06 00:32:22] epoch[13](2000/20000) Loss: 1.1604 Acc@1: 72.94 Acc@5: 89.73 time: 356.1660
[2021-11-06 00:38:16] epoch[13](4000/20000) Loss: 1.1559 Acc@1: 73.09 Acc@5: 89.80 time: 353.1072
[2021-11-06 00:44:09] epoch[13](6000/20000) Loss: 1.1588 Acc@1: 73.02 Acc@5: 89.78 time: 353.3152
[2021-11-06 00:50:02] epoch[13](8000/20000) Loss: 1.1589 Acc@1: 73.02 Acc@5: 89.81 time: 353.4816
[2021-11-06 00:55:56] epoch[13](10000/20000) Loss: 1.1598 Acc@1: 72.98 Acc@5: 89.79 time: 353.3210
[2021-11-06 01:01:49] epoch[13](12000/20000) Loss: 1.1608 Acc@1: 72.93 Acc@5: 89.78 time: 353.2187
[2021-11-06 01:07:42] epoch[13](14000/20000) Loss: 1.1611 Acc@1: 72.92 Acc@5: 89.77 time: 353.1570
[2021-11-06 01:13:35] epoch[13](16000/20000) Loss: 1.1618 Acc@1: 72.91 Acc@5: 89.76 time: 353.3862
[2021-11-06 01:19:29] epoch[13](18000/20000) Loss: 1.1623 Acc@1: 72.91 Acc@5: 89.75 time: 353.3007
[2021-11-06 01:25:22] epoch[13](20000/20000) Loss: 1.1625 Acc@1: 72.90 Acc@5: 89.74 time: 353.3772
[2021-11-06 01:25:26] train    Loss: 1.1625 Acc@1: 72.90 Acc@5: 89.74 time: 3539.2335
[2021-11-06 01:27:06] validate Loss: 1.0191 Acc@1: 74.19 Acc@5: 92.08 time: 99.9962
[2021-11-06 01:27:06] epoch 14 learning_rate 0.0001 
[2021-11-06 01:33:02] epoch[14](2000/20000) Loss: 1.1575 Acc@1: 73.09 Acc@5: 89.79 time: 356.3372
[2021-11-06 01:38:55] epoch[14](4000/20000) Loss: 1.1599 Acc@1: 73.05 Acc@5: 89.76 time: 353.1823
[2021-11-06 01:44:48] epoch[14](6000/20000) Loss: 1.1571 Acc@1: 73.12 Acc@5: 89.80 time: 353.2302
[2021-11-06 01:50:42] epoch[14](8000/20000) Loss: 1.1559 Acc@1: 73.14 Acc@5: 89.81 time: 353.3786
[2021-11-06 01:56:35] epoch[14](10000/20000) Loss: 1.1542 Acc@1: 73.17 Acc@5: 89.85 time: 353.2035
[2021-11-06 02:02:28] epoch[14](12000/20000) Loss: 1.1563 Acc@1: 73.11 Acc@5: 89.82 time: 353.1060
[2021-11-06 02:08:21] epoch[14](14000/20000) Loss: 1.1559 Acc@1: 73.09 Acc@5: 89.83 time: 353.2933
[2021-11-06 02:14:15] epoch[14](16000/20000) Loss: 1.1566 Acc@1: 73.08 Acc@5: 89.81 time: 353.3771
[2021-11-06 02:20:08] epoch[14](18000/20000) Loss: 1.1579 Acc@1: 73.04 Acc@5: 89.80 time: 353.5538
[2021-11-06 02:26:01] epoch[14](20000/20000) Loss: 1.1588 Acc@1: 73.00 Acc@5: 89.79 time: 353.2807
[2021-11-06 02:26:05] train    Loss: 1.1588 Acc@1: 73.00 Acc@5: 89.79 time: 3539.3300
[2021-11-06 02:27:45] validate Loss: 1.0243 Acc@1: 74.28 Acc@5: 92.01 time: 100.2109
[2021-11-06 02:27:45] epoch 15 learning_rate 1e-05 
[2021-11-06 02:33:41] epoch[15](2000/20000) Loss: 1.1524 Acc@1: 73.34 Acc@5: 89.77 time: 356.4129
[2021-11-06 02:39:35] epoch[15](4000/20000) Loss: 1.1430 Acc@1: 73.41 Acc@5: 89.94 time: 353.2551
[2021-11-06 02:45:28] epoch[15](6000/20000) Loss: 1.1387 Acc@1: 73.52 Acc@5: 90.04 time: 353.1870
[2021-11-06 02:51:21] epoch[15](8000/20000) Loss: 1.1376 Acc@1: 73.53 Acc@5: 90.07 time: 353.1298
[2021-11-06 02:57:14] epoch[15](10000/20000) Loss: 1.1366 Acc@1: 73.55 Acc@5: 90.07 time: 353.1351
[2021-11-06 03:03:07] epoch[15](12000/20000) Loss: 1.1360 Acc@1: 73.55 Acc@5: 90.08 time: 353.1319
[2021-11-06 03:09:01] epoch[15](14000/20000) Loss: 1.1350 Acc@1: 73.59 Acc@5: 90.10 time: 353.2373
[2021-11-06 03:14:54] epoch[15](16000/20000) Loss: 1.1355 Acc@1: 73.56 Acc@5: 90.09 time: 353.2053
[2021-11-06 03:20:47] epoch[15](18000/20000) Loss: 1.1351 Acc@1: 73.57 Acc@5: 90.10 time: 353.1022
[2021-11-06 03:26:40] epoch[15](20000/20000) Loss: 1.1351 Acc@1: 73.57 Acc@5: 90.09 time: 352.9399
[2021-11-06 03:26:43] train    Loss: 1.1351 Acc@1: 73.57 Acc@5: 90.09 time: 3538.1011
[2021-11-06 03:28:22] validate Loss: 1.0067 Acc@1: 74.57 Acc@5: 92.20 time: 98.9071
[2021-11-06 03:28:22] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-06 03:28:22] epoch 16 learning_rate 1e-05 
[2021-11-06 03:34:19] epoch[16](2000/20000) Loss: 1.1260 Acc@1: 73.72 Acc@5: 90.08 time: 356.0445
[2021-11-06 03:40:12] epoch[16](4000/20000) Loss: 1.1266 Acc@1: 73.80 Acc@5: 90.10 time: 353.2475
[2021-11-06 03:46:05] epoch[16](6000/20000) Loss: 1.1292 Acc@1: 73.69 Acc@5: 90.11 time: 353.1757
[2021-11-06 03:51:58] epoch[16](8000/20000) Loss: 1.1301 Acc@1: 73.64 Acc@5: 90.11 time: 353.3040
[2021-11-06 03:57:51] epoch[16](10000/20000) Loss: 1.1286 Acc@1: 73.69 Acc@5: 90.15 time: 352.9303
[2021-11-06 04:03:44] epoch[16](12000/20000) Loss: 1.1294 Acc@1: 73.70 Acc@5: 90.13 time: 353.1088
[2021-11-06 04:09:37] epoch[16](14000/20000) Loss: 1.1290 Acc@1: 73.68 Acc@5: 90.14 time: 353.0105
[2021-11-06 04:15:30] epoch[16](16000/20000) Loss: 1.1296 Acc@1: 73.69 Acc@5: 90.13 time: 352.9891
[2021-11-06 04:21:23] epoch[16](18000/20000) Loss: 1.1290 Acc@1: 73.70 Acc@5: 90.14 time: 353.0496
[2021-11-06 04:27:17] epoch[16](20000/20000) Loss: 1.1289 Acc@1: 73.69 Acc@5: 90.15 time: 353.1905
[2021-11-06 04:27:20] train    Loss: 1.1289 Acc@1: 73.69 Acc@5: 90.15 time: 3537.4275
[2021-11-06 04:28:58] validate Loss: 1.0078 Acc@1: 74.52 Acc@5: 92.22 time: 97.7647
[2021-11-06 04:28:58] epoch 17 learning_rate 1e-05 
[2021-11-06 04:34:54] epoch[17](2000/20000) Loss: 1.1325 Acc@1: 73.64 Acc@5: 90.09 time: 356.0951
[2021-11-06 04:40:47] epoch[17](4000/20000) Loss: 1.1309 Acc@1: 73.68 Acc@5: 90.10 time: 353.1305
[2021-11-06 04:46:40] epoch[17](6000/20000) Loss: 1.1291 Acc@1: 73.72 Acc@5: 90.14 time: 353.1818
[2021-11-06 04:52:33] epoch[17](8000/20000) Loss: 1.1277 Acc@1: 73.74 Acc@5: 90.15 time: 353.1646
[2021-11-06 04:58:26] epoch[17](10000/20000) Loss: 1.1275 Acc@1: 73.75 Acc@5: 90.16 time: 353.1093
[2021-11-06 05:04:19] epoch[17](12000/20000) Loss: 1.1265 Acc@1: 73.76 Acc@5: 90.18 time: 352.9789
[2021-11-06 05:10:13] epoch[17](14000/20000) Loss: 1.1249 Acc@1: 73.79 Acc@5: 90.21 time: 353.1822
[2021-11-06 05:16:06] epoch[17](16000/20000) Loss: 1.1247 Acc@1: 73.78 Acc@5: 90.20 time: 353.2231
[2021-11-06 05:21:59] epoch[17](18000/20000) Loss: 1.1249 Acc@1: 73.78 Acc@5: 90.20 time: 353.1302
[2021-11-06 05:27:52] epoch[17](20000/20000) Loss: 1.1250 Acc@1: 73.77 Acc@5: 90.20 time: 353.1172
[2021-11-06 05:27:55] train    Loss: 1.1250 Acc@1: 73.77 Acc@5: 90.20 time: 3537.7438
[2021-11-06 05:29:35] validate Loss: 1.0037 Acc@1: 74.62 Acc@5: 92.18 time: 99.1468
[2021-11-06 05:29:35] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-06 05:29:35] epoch 18 learning_rate 1e-05 
[2021-11-06 05:35:31] epoch[18](2000/20000) Loss: 1.1272 Acc@1: 73.74 Acc@5: 90.12 time: 356.0254
[2021-11-06 05:41:24] epoch[18](4000/20000) Loss: 1.1225 Acc@1: 73.82 Acc@5: 90.23 time: 353.1764
[2021-11-06 05:47:17] epoch[18](6000/20000) Loss: 1.1261 Acc@1: 73.71 Acc@5: 90.19 time: 353.1615
[2021-11-06 05:53:10] epoch[18](8000/20000) Loss: 1.1247 Acc@1: 73.74 Acc@5: 90.21 time: 353.0399
[2021-11-06 05:59:03] epoch[18](10000/20000) Loss: 1.1242 Acc@1: 73.75 Acc@5: 90.20 time: 353.0987
[2021-11-06 06:04:57] epoch[18](12000/20000) Loss: 1.1230 Acc@1: 73.76 Acc@5: 90.23 time: 353.0891
[2021-11-06 06:10:49] epoch[18](14000/20000) Loss: 1.1235 Acc@1: 73.75 Acc@5: 90.23 time: 352.9256
[2021-11-06 06:16:42] epoch[18](16000/20000) Loss: 1.1232 Acc@1: 73.76 Acc@5: 90.24 time: 352.9734
[2021-11-06 06:22:36] epoch[18](18000/20000) Loss: 1.1233 Acc@1: 73.77 Acc@5: 90.24 time: 353.0921
[2021-11-06 06:28:29] epoch[18](20000/20000) Loss: 1.1227 Acc@1: 73.78 Acc@5: 90.23 time: 352.9697
[2021-11-06 06:28:32] train    Loss: 1.1227 Acc@1: 73.78 Acc@5: 90.23 time: 3536.9358
[2021-11-06 06:30:09] validate Loss: 1.0024 Acc@1: 74.64 Acc@5: 92.22 time: 97.3247
[2021-11-06 06:30:10] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-06 06:30:10] epoch 19 learning_rate 1e-05 
[2021-11-06 06:36:05] epoch[19](2000/20000) Loss: 1.1237 Acc@1: 73.79 Acc@5: 90.15 time: 355.8998
[2021-11-06 06:41:59] epoch[19](4000/20000) Loss: 1.1226 Acc@1: 73.79 Acc@5: 90.17 time: 353.2423
[2021-11-06 06:47:52] epoch[19](6000/20000) Loss: 1.1190 Acc@1: 73.88 Acc@5: 90.26 time: 353.2228
[2021-11-06 06:53:45] epoch[19](8000/20000) Loss: 1.1205 Acc@1: 73.84 Acc@5: 90.24 time: 353.2092
[2021-11-06 06:59:39] epoch[19](10000/20000) Loss: 1.1199 Acc@1: 73.85 Acc@5: 90.24 time: 353.3947
[2021-11-06 07:05:32] epoch[19](12000/20000) Loss: 1.1207 Acc@1: 73.86 Acc@5: 90.24 time: 353.2434
[2021-11-06 07:11:25] epoch[19](14000/20000) Loss: 1.1210 Acc@1: 73.87 Acc@5: 90.23 time: 352.9814
[2021-11-06 07:17:18] epoch[19](16000/20000) Loss: 1.1215 Acc@1: 73.86 Acc@5: 90.23 time: 353.0601
[2021-11-06 07:23:11] epoch[19](18000/20000) Loss: 1.1213 Acc@1: 73.86 Acc@5: 90.24 time: 353.1340
[2021-11-06 07:29:04] epoch[19](20000/20000) Loss: 1.1218 Acc@1: 73.85 Acc@5: 90.23 time: 353.3418
[2021-11-06 07:29:08] train    Loss: 1.1219 Acc@1: 73.85 Acc@5: 90.23 time: 3538.1287
[2021-11-06 07:30:45] validate Loss: 1.0036 Acc@1: 74.66 Acc@5: 92.19 time: 97.5739
[2021-11-06 07:30:46] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-06 07:30:46] epoch 20 learning_rate 1e-05 
[2021-11-06 07:36:42] epoch[20](2000/20000) Loss: 1.1260 Acc@1: 73.67 Acc@5: 90.29 time: 356.0015
[2021-11-06 07:42:35] epoch[20](4000/20000) Loss: 1.1257 Acc@1: 73.74 Acc@5: 90.22 time: 353.2245
[2021-11-06 07:48:28] epoch[20](6000/20000) Loss: 1.1231 Acc@1: 73.77 Acc@5: 90.23 time: 352.9868
[2021-11-06 07:54:21] epoch[20](8000/20000) Loss: 1.1211 Acc@1: 73.79 Acc@5: 90.26 time: 353.1076
[2021-11-06 08:00:14] epoch[20](10000/20000) Loss: 1.1224 Acc@1: 73.78 Acc@5: 90.24 time: 352.9649
[2021-11-06 08:06:07] epoch[20](12000/20000) Loss: 1.1220 Acc@1: 73.80 Acc@5: 90.23 time: 353.1992
[2021-11-06 08:12:01] epoch[20](14000/20000) Loss: 1.1218 Acc@1: 73.82 Acc@5: 90.23 time: 353.3473
[2021-11-06 08:17:54] epoch[20](16000/20000) Loss: 1.1224 Acc@1: 73.82 Acc@5: 90.22 time: 353.3280
[2021-11-06 08:23:47] epoch[20](18000/20000) Loss: 1.1221 Acc@1: 73.81 Acc@5: 90.23 time: 353.1148
[2021-11-06 08:29:40] epoch[20](20000/20000) Loss: 1.1222 Acc@1: 73.81 Acc@5: 90.23 time: 353.0673
[2021-11-06 08:29:43] train    Loss: 1.1222 Acc@1: 73.81 Acc@5: 90.23 time: 3537.7156
[2021-11-06 08:31:22] validate Loss: 1.0027 Acc@1: 74.64 Acc@5: 92.24 time: 98.4230
[2021-11-06 08:31:22] epoch 21 learning_rate 1e-05 
[2021-11-06 08:37:18] epoch[21](2000/20000) Loss: 1.1160 Acc@1: 73.88 Acc@5: 90.37 time: 355.9945
[2021-11-06 08:43:11] epoch[21](4000/20000) Loss: 1.1178 Acc@1: 73.96 Acc@5: 90.28 time: 352.9772
[2021-11-06 08:49:04] epoch[21](6000/20000) Loss: 1.1161 Acc@1: 73.96 Acc@5: 90.29 time: 353.3095
[2021-11-06 08:54:57] epoch[21](8000/20000) Loss: 1.1166 Acc@1: 73.93 Acc@5: 90.31 time: 353.0542
[2021-11-06 09:00:50] epoch[21](10000/20000) Loss: 1.1172 Acc@1: 73.95 Acc@5: 90.31 time: 353.0180
[2021-11-06 09:06:43] epoch[21](12000/20000) Loss: 1.1182 Acc@1: 73.93 Acc@5: 90.29 time: 353.0657
[2021-11-06 09:12:36] epoch[21](14000/20000) Loss: 1.1182 Acc@1: 73.91 Acc@5: 90.29 time: 352.9814
[2021-11-06 09:18:29] epoch[21](16000/20000) Loss: 1.1179 Acc@1: 73.92 Acc@5: 90.29 time: 352.7589
[2021-11-06 09:24:22] epoch[21](18000/20000) Loss: 1.1180 Acc@1: 73.91 Acc@5: 90.29 time: 353.0761
[2021-11-06 09:30:15] epoch[21](20000/20000) Loss: 1.1183 Acc@1: 73.90 Acc@5: 90.28 time: 352.7681
[2021-11-06 09:30:18] train    Loss: 1.1183 Acc@1: 73.90 Acc@5: 90.28 time: 3536.3881
[2021-11-06 09:31:53] validate Loss: 1.0035 Acc@1: 74.59 Acc@5: 92.19 time: 94.9867
[2021-11-06 09:31:53] epoch 22 learning_rate 1e-05 
[2021-11-06 09:37:49] epoch[22](2000/20000) Loss: 1.1177 Acc@1: 73.91 Acc@5: 90.32 time: 355.7317
[2021-11-06 09:43:42] epoch[22](4000/20000) Loss: 1.1170 Acc@1: 73.94 Acc@5: 90.34 time: 352.9359
[2021-11-06 09:49:35] epoch[22](6000/20000) Loss: 1.1177 Acc@1: 73.92 Acc@5: 90.31 time: 352.7081
[2021-11-06 09:55:27] epoch[22](8000/20000) Loss: 1.1184 Acc@1: 73.88 Acc@5: 90.30 time: 352.1460
[2021-11-06 10:01:19] epoch[22](10000/20000) Loss: 1.1183 Acc@1: 73.91 Acc@5: 90.28 time: 352.1108
[2021-11-06 10:07:11] epoch[22](12000/20000) Loss: 1.1189 Acc@1: 73.91 Acc@5: 90.26 time: 352.3667
[2021-11-06 10:13:03] epoch[22](14000/20000) Loss: 1.1204 Acc@1: 73.89 Acc@5: 90.24 time: 352.2088
[2021-11-06 10:18:55] epoch[22](16000/20000) Loss: 1.1192 Acc@1: 73.92 Acc@5: 90.27 time: 352.0214
[2021-11-06 10:24:48] epoch[22](18000/20000) Loss: 1.1187 Acc@1: 73.93 Acc@5: 90.27 time: 352.2526
[2021-11-06 10:30:40] epoch[22](20000/20000) Loss: 1.1186 Acc@1: 73.93 Acc@5: 90.28 time: 352.0454
[2021-11-06 10:30:43] train    Loss: 1.1186 Acc@1: 73.93 Acc@5: 90.28 time: 3529.8826
[2021-11-06 10:32:13] validate Loss: 1.0018 Acc@1: 74.69 Acc@5: 92.27 time: 90.1475
[2021-11-06 10:32:14] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-06 10:32:14] epoch 23 learning_rate 1e-05 
[2021-11-06 10:38:08] epoch[23](2000/20000) Loss: 1.1193 Acc@1: 73.89 Acc@5: 90.35 time: 354.8845
[2021-11-06 10:44:01] epoch[23](4000/20000) Loss: 1.1160 Acc@1: 73.93 Acc@5: 90.39 time: 352.0678
[2021-11-06 10:49:53] epoch[23](6000/20000) Loss: 1.1152 Acc@1: 74.00 Acc@5: 90.35 time: 352.3028
[2021-11-06 10:55:45] epoch[23](8000/20000) Loss: 1.1177 Acc@1: 73.94 Acc@5: 90.31 time: 352.1928
[2021-11-06 11:01:37] epoch[23](10000/20000) Loss: 1.1174 Acc@1: 73.98 Acc@5: 90.32 time: 352.2626
[2021-11-06 11:07:29] epoch[23](12000/20000) Loss: 1.1201 Acc@1: 73.92 Acc@5: 90.28 time: 352.1478
[2021-11-06 11:13:22] epoch[23](14000/20000) Loss: 1.1194 Acc@1: 73.92 Acc@5: 90.28 time: 352.2897
[2021-11-06 11:19:14] epoch[23](16000/20000) Loss: 1.1190 Acc@1: 73.94 Acc@5: 90.28 time: 352.1885
[2021-11-06 11:25:06] epoch[23](18000/20000) Loss: 1.1188 Acc@1: 73.95 Acc@5: 90.29 time: 352.3848
[2021-11-06 11:30:58] epoch[23](20000/20000) Loss: 1.1191 Acc@1: 73.93 Acc@5: 90.28 time: 352.0299
[2021-11-06 11:31:02] train    Loss: 1.1191 Acc@1: 73.93 Acc@5: 90.28 time: 3528.1113
[2021-11-06 11:32:32] validate Loss: 1.0011 Acc@1: 74.73 Acc@5: 92.26 time: 89.8020
[2021-11-06 11:32:32] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-06 11:32:32] epoch 24 learning_rate 1e-05 
[2021-11-06 11:38:27] epoch[24](2000/20000) Loss: 1.1134 Acc@1: 74.06 Acc@5: 90.34 time: 355.1183
[2021-11-06 11:44:19] epoch[24](4000/20000) Loss: 1.1152 Acc@1: 74.02 Acc@5: 90.29 time: 352.2785
[2021-11-06 11:50:11] epoch[24](6000/20000) Loss: 1.1139 Acc@1: 74.04 Acc@5: 90.32 time: 352.0076
[2021-11-06 11:56:04] epoch[24](8000/20000) Loss: 1.1153 Acc@1: 73.98 Acc@5: 90.31 time: 352.2679
[2021-11-06 12:01:56] epoch[24](10000/20000) Loss: 1.1173 Acc@1: 73.97 Acc@5: 90.28 time: 352.2087
[2021-11-06 12:07:48] epoch[24](12000/20000) Loss: 1.1180 Acc@1: 73.95 Acc@5: 90.28 time: 352.2200
[2021-11-06 12:13:40] epoch[24](14000/20000) Loss: 1.1174 Acc@1: 73.97 Acc@5: 90.29 time: 352.3445
[2021-11-06 12:19:33] epoch[24](16000/20000) Loss: 1.1179 Acc@1: 73.94 Acc@5: 90.28 time: 352.1763
[2021-11-06 12:25:25] epoch[24](18000/20000) Loss: 1.1179 Acc@1: 73.93 Acc@5: 90.28 time: 352.1849
[2021-11-06 12:31:17] epoch[24](20000/20000) Loss: 1.1184 Acc@1: 73.91 Acc@5: 90.27 time: 352.2765
[2021-11-06 12:31:20] train    Loss: 1.1185 Acc@1: 73.91 Acc@5: 90.27 time: 3528.4227
[2021-11-06 12:32:51] validate Loss: 1.0002 Acc@1: 74.72 Acc@5: 92.21 time: 90.9407
[2021-11-06 12:32:51] epoch 25 learning_rate 1e-05 
[2021-11-06 12:38:46] epoch[25](2000/20000) Loss: 1.1139 Acc@1: 73.98 Acc@5: 90.33 time: 355.1863
[2021-11-06 12:44:39] epoch[25](4000/20000) Loss: 1.1161 Acc@1: 73.95 Acc@5: 90.32 time: 352.2784
[2021-11-06 12:50:31] epoch[25](6000/20000) Loss: 1.1143 Acc@1: 73.98 Acc@5: 90.34 time: 352.3052
[2021-11-06 12:56:23] epoch[25](8000/20000) Loss: 1.1142 Acc@1: 73.97 Acc@5: 90.36 time: 352.2145
[2021-11-06 13:02:15] epoch[25](10000/20000) Loss: 1.1159 Acc@1: 73.94 Acc@5: 90.34 time: 352.1765
[2021-11-06 13:08:08] epoch[25](12000/20000) Loss: 1.1169 Acc@1: 73.92 Acc@5: 90.32 time: 352.1974
[2021-11-06 13:14:00] epoch[25](14000/20000) Loss: 1.1170 Acc@1: 73.92 Acc@5: 90.31 time: 352.1287
[2021-11-06 13:19:52] epoch[25](16000/20000) Loss: 1.1169 Acc@1: 73.92 Acc@5: 90.31 time: 352.2972
[2021-11-06 13:25:44] epoch[25](18000/20000) Loss: 1.1170 Acc@1: 73.93 Acc@5: 90.30 time: 352.1661
[2021-11-06 13:31:36] epoch[25](20000/20000) Loss: 1.1170 Acc@1: 73.93 Acc@5: 90.30 time: 352.0734
[2021-11-06 13:31:40] train    Loss: 1.1171 Acc@1: 73.93 Acc@5: 90.30 time: 3528.3992
[2021-11-06 13:33:10] validate Loss: 0.9989 Acc@1: 74.68 Acc@5: 92.30 time: 90.1504
[2021-11-06 13:33:10] epoch 26 learning_rate 1e-05 
[2021-11-06 13:39:05] epoch[26](2000/20000) Loss: 1.1051 Acc@1: 74.19 Acc@5: 90.47 time: 355.1126
[2021-11-06 13:44:57] epoch[26](4000/20000) Loss: 1.1082 Acc@1: 74.16 Acc@5: 90.41 time: 352.0687
[2021-11-06 13:50:49] epoch[26](6000/20000) Loss: 1.1097 Acc@1: 74.10 Acc@5: 90.38 time: 352.1246
[2021-11-06 13:56:41] epoch[26](8000/20000) Loss: 1.1119 Acc@1: 74.05 Acc@5: 90.36 time: 351.9795
[2021-11-06 14:02:34] epoch[26](10000/20000) Loss: 1.1123 Acc@1: 74.01 Acc@5: 90.35 time: 352.3651
[2021-11-06 14:08:26] epoch[26](12000/20000) Loss: 1.1133 Acc@1: 73.99 Acc@5: 90.34 time: 352.1838
[2021-11-06 14:14:18] epoch[26](14000/20000) Loss: 1.1135 Acc@1: 73.98 Acc@5: 90.35 time: 352.1009
[2021-11-06 14:20:10] epoch[26](16000/20000) Loss: 1.1145 Acc@1: 73.96 Acc@5: 90.33 time: 352.2155
[2021-11-06 14:26:02] epoch[26](18000/20000) Loss: 1.1153 Acc@1: 73.97 Acc@5: 90.31 time: 352.1709
[2021-11-06 14:31:55] epoch[26](20000/20000) Loss: 1.1156 Acc@1: 73.97 Acc@5: 90.29 time: 352.4398
[2021-11-06 14:31:58] train    Loss: 1.1156 Acc@1: 73.97 Acc@5: 90.29 time: 3528.1409
[2021-11-06 14:33:28] validate Loss: 1.0004 Acc@1: 74.70 Acc@5: 92.20 time: 90.0803
[2021-11-06 14:33:28] epoch 27 learning_rate 1e-05 
[2021-11-06 14:39:23] epoch[27](2000/20000) Loss: 1.1119 Acc@1: 73.98 Acc@5: 90.48 time: 355.1215
[2021-11-06 14:45:15] epoch[27](4000/20000) Loss: 1.1123 Acc@1: 73.97 Acc@5: 90.43 time: 352.0233
[2021-11-06 14:51:07] epoch[27](6000/20000) Loss: 1.1122 Acc@1: 74.00 Acc@5: 90.40 time: 352.1895
[2021-11-06 14:57:00] epoch[27](8000/20000) Loss: 1.1121 Acc@1: 74.01 Acc@5: 90.38 time: 352.1677
[2021-11-06 15:02:51] epoch[27](10000/20000) Loss: 1.1120 Acc@1: 74.04 Acc@5: 90.38 time: 351.8856
[2021-11-06 15:08:44] epoch[27](12000/20000) Loss: 1.1115 Acc@1: 74.04 Acc@5: 90.39 time: 352.1433
[2021-11-06 15:14:36] epoch[27](14000/20000) Loss: 1.1123 Acc@1: 74.02 Acc@5: 90.38 time: 352.0774
[2021-11-06 15:20:28] epoch[27](16000/20000) Loss: 1.1125 Acc@1: 74.02 Acc@5: 90.37 time: 352.1243
[2021-11-06 15:26:20] epoch[27](18000/20000) Loss: 1.1134 Acc@1: 74.01 Acc@5: 90.35 time: 352.1816
[2021-11-06 15:32:12] epoch[27](20000/20000) Loss: 1.1136 Acc@1: 74.01 Acc@5: 90.34 time: 352.2582
[2021-11-06 15:32:16] train    Loss: 1.1136 Acc@1: 74.01 Acc@5: 90.34 time: 3527.5580
[2021-11-06 15:33:46] validate Loss: 1.0003 Acc@1: 74.78 Acc@5: 92.27 time: 90.8507
[2021-11-06 15:33:47] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-06 15:33:47] epoch 28 learning_rate 1e-05 
[2021-11-06 15:39:42] epoch[28](2000/20000) Loss: 1.1147 Acc@1: 73.87 Acc@5: 90.29 time: 354.9313
[2021-11-06 15:45:34] epoch[28](4000/20000) Loss: 1.1180 Acc@1: 73.87 Acc@5: 90.29 time: 352.3813
[2021-11-06 15:51:26] epoch[28](6000/20000) Loss: 1.1154 Acc@1: 73.94 Acc@5: 90.33 time: 352.3106
[2021-11-06 15:57:19] epoch[28](8000/20000) Loss: 1.1138 Acc@1: 73.99 Acc@5: 90.34 time: 352.2860
[2021-11-06 16:03:11] epoch[28](10000/20000) Loss: 1.1137 Acc@1: 73.99 Acc@5: 90.36 time: 352.3909
[2021-11-06 16:09:03] epoch[28](12000/20000) Loss: 1.1133 Acc@1: 73.99 Acc@5: 90.37 time: 352.2651
[2021-11-06 16:14:56] epoch[28](14000/20000) Loss: 1.1145 Acc@1: 73.97 Acc@5: 90.34 time: 352.1854
[2021-11-06 16:20:48] epoch[28](16000/20000) Loss: 1.1146 Acc@1: 73.98 Acc@5: 90.35 time: 352.1950
[2021-11-06 16:26:40] epoch[28](18000/20000) Loss: 1.1151 Acc@1: 73.96 Acc@5: 90.33 time: 352.3881
[2021-11-06 16:32:32] epoch[28](20000/20000) Loss: 1.1153 Acc@1: 73.97 Acc@5: 90.32 time: 352.1201
[2021-11-06 16:32:36] train    Loss: 1.1153 Acc@1: 73.97 Acc@5: 90.32 time: 3528.8279
[2021-11-06 16:34:07] validate Loss: 0.9997 Acc@1: 74.76 Acc@5: 92.26 time: 90.9037
[2021-11-06 16:34:07] epoch 29 learning_rate 1e-05 
[2021-11-06 16:40:02] epoch[29](2000/20000) Loss: 1.1105 Acc@1: 74.18 Acc@5: 90.33 time: 355.0331
[2021-11-06 16:45:54] epoch[29](4000/20000) Loss: 1.1134 Acc@1: 74.06 Acc@5: 90.29 time: 352.3840
[2021-11-06 16:51:47] epoch[29](6000/20000) Loss: 1.1144 Acc@1: 74.05 Acc@5: 90.28 time: 352.5441
[2021-11-06 16:57:39] epoch[29](8000/20000) Loss: 1.1133 Acc@1: 74.06 Acc@5: 90.31 time: 352.4710
[2021-11-06 17:03:31] epoch[29](10000/20000) Loss: 1.1136 Acc@1: 74.04 Acc@5: 90.30 time: 352.1923
[2021-11-06 17:09:24] epoch[29](12000/20000) Loss: 1.1137 Acc@1: 74.04 Acc@5: 90.30 time: 352.2772
[2021-11-06 17:15:16] epoch[29](14000/20000) Loss: 1.1142 Acc@1: 74.02 Acc@5: 90.30 time: 352.3807
[2021-11-06 17:21:08] epoch[29](16000/20000) Loss: 1.1146 Acc@1: 74.01 Acc@5: 90.29 time: 352.3471
[2021-11-06 17:27:00] epoch[29](18000/20000) Loss: 1.1142 Acc@1: 74.02 Acc@5: 90.30 time: 352.2594
[2021-11-06 17:32:53] epoch[29](20000/20000) Loss: 1.1144 Acc@1: 74.01 Acc@5: 90.30 time: 352.2623
[2021-11-06 17:32:56] train    Loss: 1.1144 Acc@1: 74.01 Acc@5: 90.30 time: 3529.5453
[2021-11-06 17:34:29] validate Loss: 0.9985 Acc@1: 74.80 Acc@5: 92.25 time: 92.6698
[2021-11-06 17:34:29] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-06 17:36:56] ResNet50(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): ModuleList(
    (0): Bottleneck(
      (conv1): Conv2d(64, 57, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(57, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(57, 204, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 204, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(204, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(32, 204, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(204, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(32, 204, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (layer2): ModuleList(
    (0): Bottleneck(
      (conv1): Conv2d(204, 115, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(115, 115, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(115, 409, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(409, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(204, 409, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(409, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(409, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(64, 409, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(409, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(409, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(64, 409, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(409, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(409, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(64, 409, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(409, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (layer3): ModuleList(
    (0): Bottleneck(
      (conv1): Conv2d(409, 230, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(230, 230, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(230, 819, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(819, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(409, 819, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(819, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(819, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(128, 819, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(819, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(819, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(128, 819, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(819, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(819, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(128, 819, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(819, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(819, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(128, 819, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(819, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(819, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(128, 819, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(819, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (layer4): ModuleList(
    (0): Bottleneck(
      (conv1): Conv2d(819, 460, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(460, 460, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(460, 1843, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1843, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(819, 1843, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1843, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1843, 409, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(409, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(409, 409, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(409, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(409, 1843, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1843, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1843, 409, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(409, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(409, 409, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(409, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(409, 1843, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1843, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=1843, out_features=1000, bias=True)
)
[2021-11-06 17:36:56] storing pruned_model:/finally_pruned_model/resnet_50_11_fs.pt
[2021-11-06 17:38:20] validate Loss: 0.9985 Acc@1: 74.81 Acc@5: 92.25 time: 83.4090
[2021-11-06 17:38:20] finally model  Acc@1: 74.81 Acc@5: 92.25 flops: 2134021533.0 params:16082147.0
