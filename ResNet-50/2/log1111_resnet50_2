[2021-11-10 02:53:49] args:Namespace(ablation_id=0, arch='resnet_50', batch_size=64, compress_rate='[0.]+[0.1,0.1,0.4]*1+[0.7,0.7,0.4]*2+[0.2,0.2,0.4]*1+[0.7,0.7,0.4]*3+[0.2,0.2,0.3]*1+[0.7,0.7,0.3]*5+[0.1,0.1,0.1]+[0.2,0.3,0.1]*2', data_dir='/home/featurize/data', dataset='ImageNet', epochs=30, from_scratch=True, gpu=0, input_size=224, job_dir='/home/featurize/work/CCCrank5', lr=0.001, lr_decay_step='5,20', momentum=0.9, num_workers=8, pretrained=False, resume='pruned_checkpoint/resnet_50_cov52.pt', save_id=111, start_cov=-1, weight_decay=0.0005)
[2021-11-10 02:53:49] loading checkpoint:pruned_checkpoint/resnet_50_cov52.pt
[2021-11-10 02:53:49] loading ranks form checkpoint...
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv0.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv1.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv2.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv3.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv3.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv4.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv5.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv6.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv7.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv8.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv9.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv10.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv11.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv12.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv12.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv13.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv14.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv15.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv16.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv17.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv18.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv19.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv20.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv21.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv22.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv23.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv24.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv24.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv25.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv26.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv27.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv28.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv29.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv30.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv31.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv32.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv33.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv34.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv35.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv36.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv37.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv38.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv39.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv40.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv41.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv42.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv42.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv43.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv44.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv45.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv46.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv47.npy
[2021-11-10 02:53:49] loading rank_conv/resnet_50/rank_conv48.npy
[2021-11-10 02:55:15] validate Loss: 1.1472 Acc@1: 71.62 Acc@5: 90.41 time: 85.3678
[2021-11-10 02:55:15] epoch 0 learning_rate 0.001 
[2021-11-10 03:01:14] epoch[0](2000/20000) Loss: 1.3798 Acc@1: 67.23 Acc@5: 86.48 time: 359.0587
[2021-11-10 03:07:10] epoch[0](4000/20000) Loss: 1.3937 Acc@1: 66.97 Acc@5: 86.37 time: 355.8722
[2021-11-10 03:13:06] epoch[0](6000/20000) Loss: 1.4050 Acc@1: 66.76 Acc@5: 86.21 time: 356.2160
[2021-11-10 03:19:02] epoch[0](8000/20000) Loss: 1.4089 Acc@1: 66.67 Acc@5: 86.13 time: 356.1968
[2021-11-10 03:24:58] epoch[0](10000/20000) Loss: 1.4139 Acc@1: 66.55 Acc@5: 86.08 time: 355.7485
[2021-11-10 03:30:54] epoch[0](12000/20000) Loss: 1.4173 Acc@1: 66.49 Acc@5: 86.04 time: 356.0326
[2021-11-10 03:36:49] epoch[0](14000/20000) Loss: 1.4218 Acc@1: 66.41 Acc@5: 85.97 time: 355.5635
[2021-11-10 03:42:46] epoch[0](16000/20000) Loss: 1.4242 Acc@1: 66.36 Acc@5: 85.94 time: 356.2753
[2021-11-10 03:48:41] epoch[0](18000/20000) Loss: 1.4257 Acc@1: 66.33 Acc@5: 85.92 time: 355.7816
[2021-11-10 03:54:37] epoch[0](20000/20000) Loss: 1.4277 Acc@1: 66.28 Acc@5: 85.90 time: 355.8744
[2021-11-10 03:54:42] train    Loss: 1.4276 Acc@1: 66.28 Acc@5: 85.90 time: 3567.2048
[2021-11-10 03:56:23] validate Loss: 1.2327 Acc@1: 69.47 Acc@5: 89.26 time: 101.5918
[2021-11-10 03:56:24] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-10 03:56:24] epoch 1 learning_rate 0.001 
[2021-11-10 04:02:22] epoch[1](2000/20000) Loss: 1.4165 Acc@1: 66.69 Acc@5: 86.15 time: 358.3716
[2021-11-10 04:08:19] epoch[1](4000/20000) Loss: 1.4221 Acc@1: 66.56 Acc@5: 86.07 time: 356.3802
[2021-11-10 04:14:14] epoch[1](6000/20000) Loss: 1.4231 Acc@1: 66.54 Acc@5: 86.09 time: 355.6030
[2021-11-10 04:20:10] epoch[1](8000/20000) Loss: 1.4287 Acc@1: 66.42 Acc@5: 86.03 time: 355.6774
[2021-11-10 04:26:06] epoch[1](10000/20000) Loss: 1.4337 Acc@1: 66.32 Acc@5: 85.98 time: 356.1914
[2021-11-10 04:32:02] epoch[1](12000/20000) Loss: 1.4379 Acc@1: 66.25 Acc@5: 85.92 time: 355.6028
[2021-11-10 04:37:58] epoch[1](14000/20000) Loss: 1.4401 Acc@1: 66.23 Acc@5: 85.90 time: 356.1588
[2021-11-10 04:43:53] epoch[1](16000/20000) Loss: 1.4443 Acc@1: 66.15 Acc@5: 85.86 time: 355.5931
[2021-11-10 04:49:50] epoch[1](18000/20000) Loss: 1.4465 Acc@1: 66.12 Acc@5: 85.84 time: 356.2861
[2021-11-10 04:55:45] epoch[1](20000/20000) Loss: 1.4501 Acc@1: 66.05 Acc@5: 85.79 time: 355.6772
[2021-11-10 04:55:49] train    Loss: 1.4501 Acc@1: 66.05 Acc@5: 85.79 time: 3565.1384
[2021-11-10 04:57:26] validate Loss: 1.2296 Acc@1: 69.44 Acc@5: 89.32 time: 97.1280
[2021-11-10 04:57:26] epoch 2 learning_rate 0.001 
[2021-11-10 05:03:25] epoch[2](2000/20000) Loss: 1.4456 Acc@1: 66.12 Acc@5: 85.91 time: 359.1864
[2021-11-10 05:09:21] epoch[2](4000/20000) Loss: 1.4519 Acc@1: 66.14 Acc@5: 85.82 time: 355.6246
[2021-11-10 05:15:17] epoch[2](6000/20000) Loss: 1.4565 Acc@1: 66.02 Acc@5: 85.77 time: 356.5890
[2021-11-10 05:21:13] epoch[2](8000/20000) Loss: 1.4606 Acc@1: 65.92 Acc@5: 85.73 time: 355.6091
[2021-11-10 05:27:09] epoch[2](10000/20000) Loss: 1.4643 Acc@1: 65.85 Acc@5: 85.66 time: 355.4727
[2021-11-10 05:33:04] epoch[2](12000/20000) Loss: 1.4691 Acc@1: 65.75 Acc@5: 85.60 time: 355.9228
[2021-11-10 05:39:00] epoch[2](14000/20000) Loss: 1.4721 Acc@1: 65.71 Acc@5: 85.56 time: 355.4377
[2021-11-10 05:44:55] epoch[2](16000/20000) Loss: 1.4762 Acc@1: 65.65 Acc@5: 85.51 time: 355.6094
[2021-11-10 05:50:52] epoch[2](18000/20000) Loss: 1.4795 Acc@1: 65.59 Acc@5: 85.47 time: 356.2270
[2021-11-10 05:56:47] epoch[2](20000/20000) Loss: 1.4822 Acc@1: 65.55 Acc@5: 85.45 time: 355.5580
[2021-11-10 05:56:51] train    Loss: 1.4823 Acc@1: 65.55 Acc@5: 85.45 time: 3564.8395
[2021-11-10 05:58:28] validate Loss: 1.2584 Acc@1: 68.70 Acc@5: 89.17 time: 97.4597
[2021-11-10 05:58:28] epoch 3 learning_rate 0.001 
[2021-11-10 06:04:28] epoch[3](2000/20000) Loss: 1.4803 Acc@1: 65.85 Acc@5: 85.59 time: 359.6655
[2021-11-10 06:10:24] epoch[3](4000/20000) Loss: 1.4894 Acc@1: 65.63 Acc@5: 85.48 time: 356.0659
[2021-11-10 06:16:20] epoch[3](6000/20000) Loss: 1.4919 Acc@1: 65.48 Acc@5: 85.44 time: 355.8246
[2021-11-10 06:22:17] epoch[3](8000/20000) Loss: 1.4978 Acc@1: 65.40 Acc@5: 85.35 time: 356.6436
[2021-11-10 06:28:13] epoch[3](10000/20000) Loss: 1.4999 Acc@1: 65.32 Acc@5: 85.32 time: 356.0294
[2021-11-10 06:34:09] epoch[3](12000/20000) Loss: 1.5048 Acc@1: 65.22 Acc@5: 85.25 time: 356.0455
[2021-11-10 06:40:05] epoch[3](14000/20000) Loss: 1.5092 Acc@1: 65.11 Acc@5: 85.20 time: 356.8072
[2021-11-10 06:46:02] epoch[3](16000/20000) Loss: 1.5121 Acc@1: 65.06 Acc@5: 85.16 time: 356.3759
[2021-11-10 06:51:58] epoch[3](18000/20000) Loss: 1.5155 Acc@1: 64.98 Acc@5: 85.11 time: 356.0523
[2021-11-10 06:57:54] epoch[3](20000/20000) Loss: 1.5182 Acc@1: 64.92 Acc@5: 85.07 time: 356.4735
[2021-11-10 06:57:58] train    Loss: 1.5182 Acc@1: 64.92 Acc@5: 85.07 time: 3570.0262
[2021-11-10 06:59:38] validate Loss: 1.2904 Acc@1: 68.11 Acc@5: 88.61 time: 99.2538
[2021-11-10 06:59:38] epoch 4 learning_rate 0.001 
[2021-11-10 07:05:36] epoch[4](2000/20000) Loss: 1.5166 Acc@1: 65.02 Acc@5: 85.14 time: 358.6919
[2021-11-10 07:11:32] epoch[4](4000/20000) Loss: 1.5187 Acc@1: 64.98 Acc@5: 85.13 time: 355.8040
[2021-11-10 07:17:28] epoch[4](6000/20000) Loss: 1.5251 Acc@1: 64.84 Acc@5: 85.03 time: 356.2497
[2021-11-10 07:23:24] epoch[4](8000/20000) Loss: 1.5289 Acc@1: 64.74 Acc@5: 85.02 time: 356.0027
[2021-11-10 07:29:20] epoch[4](10000/20000) Loss: 1.5334 Acc@1: 64.66 Acc@5: 84.95 time: 355.9711
[2021-11-10 07:35:16] epoch[4](12000/20000) Loss: 1.5382 Acc@1: 64.57 Acc@5: 84.89 time: 355.7719
[2021-11-10 07:41:13] epoch[4](14000/20000) Loss: 1.5421 Acc@1: 64.49 Acc@5: 84.83 time: 356.5050
[2021-11-10 07:47:08] epoch[4](16000/20000) Loss: 1.5477 Acc@1: 64.37 Acc@5: 84.75 time: 355.5750
[2021-11-10 07:53:04] epoch[4](18000/20000) Loss: 1.5526 Acc@1: 64.28 Acc@5: 84.69 time: 355.7503
[2021-11-10 07:59:00] epoch[4](20000/20000) Loss: 1.5557 Acc@1: 64.23 Acc@5: 84.63 time: 355.9119
[2021-11-10 07:59:05] train    Loss: 1.5557 Acc@1: 64.23 Acc@5: 84.63 time: 3566.9184
[2021-11-10 08:00:45] validate Loss: 1.3185 Acc@1: 67.62 Acc@5: 88.29 time: 100.3782
[2021-11-10 08:00:45] epoch 5 learning_rate 0.0001 
[2021-11-10 08:06:44] epoch[5](2000/20000) Loss: 1.4462 Acc@1: 66.81 Acc@5: 86.12 time: 359.0612
[2021-11-10 08:12:40] epoch[5](4000/20000) Loss: 1.4263 Acc@1: 67.19 Acc@5: 86.39 time: 355.8499
[2021-11-10 08:18:36] epoch[5](6000/20000) Loss: 1.4121 Acc@1: 67.45 Acc@5: 86.57 time: 355.7342
[2021-11-10 08:24:32] epoch[5](8000/20000) Loss: 1.4030 Acc@1: 67.64 Acc@5: 86.70 time: 356.1062
[2021-11-10 08:30:28] epoch[5](10000/20000) Loss: 1.3959 Acc@1: 67.82 Acc@5: 86.78 time: 355.9391
[2021-11-10 08:36:24] epoch[5](12000/20000) Loss: 1.3892 Acc@1: 67.95 Acc@5: 86.88 time: 355.9473
[2021-11-10 08:42:19] epoch[5](14000/20000) Loss: 1.3854 Acc@1: 68.03 Acc@5: 86.92 time: 355.5695
[2021-11-10 08:48:15] epoch[5](16000/20000) Loss: 1.3822 Acc@1: 68.09 Acc@5: 86.96 time: 355.7821
[2021-11-10 08:54:11] epoch[5](18000/20000) Loss: 1.3788 Acc@1: 68.17 Acc@5: 87.00 time: 355.8211
[2021-11-10 09:00:06] epoch[5](20000/20000) Loss: 1.3756 Acc@1: 68.23 Acc@5: 87.05 time: 355.6218
[2021-11-10 09:00:10] train    Loss: 1.3756 Acc@1: 68.23 Acc@5: 87.05 time: 3564.9809
[2021-11-10 09:01:39] validate Loss: 1.1340 Acc@1: 71.82 Acc@5: 90.54 time: 89.1112
[2021-11-10 09:01:39] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-10 09:01:39] epoch 6 learning_rate 0.0001 
[2021-11-10 09:07:38] epoch[6](2000/20000) Loss: 1.3361 Acc@1: 69.10 Acc@5: 87.51 time: 358.3296
[2021-11-10 09:13:33] epoch[6](4000/20000) Loss: 1.3362 Acc@1: 69.10 Acc@5: 87.51 time: 355.4115
[2021-11-10 09:19:29] epoch[6](6000/20000) Loss: 1.3353 Acc@1: 69.15 Acc@5: 87.51 time: 355.7275
[2021-11-10 09:25:25] epoch[6](8000/20000) Loss: 1.3343 Acc@1: 69.17 Acc@5: 87.53 time: 355.9069
[2021-11-10 09:31:21] epoch[6](10000/20000) Loss: 1.3330 Acc@1: 69.20 Acc@5: 87.54 time: 356.1365
[2021-11-10 09:37:17] epoch[6](12000/20000) Loss: 1.3304 Acc@1: 69.26 Acc@5: 87.60 time: 356.1849
[2021-11-10 09:43:13] epoch[6](14000/20000) Loss: 1.3306 Acc@1: 69.23 Acc@5: 87.60 time: 355.7546
[2021-11-10 09:49:09] epoch[6](16000/20000) Loss: 1.3293 Acc@1: 69.25 Acc@5: 87.62 time: 356.4939
[2021-11-10 09:55:05] epoch[6](18000/20000) Loss: 1.3291 Acc@1: 69.25 Acc@5: 87.62 time: 355.5408
[2021-11-10 10:01:01] epoch[6](20000/20000) Loss: 1.3292 Acc@1: 69.24 Acc@5: 87.62 time: 355.7973
[2021-11-10 10:01:04] train    Loss: 1.3291 Acc@1: 69.24 Acc@5: 87.63 time: 3564.8605
[2021-11-10 10:02:39] validate Loss: 1.1164 Acc@1: 72.21 Acc@5: 90.78 time: 94.8954
[2021-11-10 10:02:40] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-10 10:02:40] epoch 7 learning_rate 0.0001 
[2021-11-10 10:08:38] epoch[7](2000/20000) Loss: 1.3161 Acc@1: 69.49 Acc@5: 87.80 time: 358.7043
[2021-11-10 10:14:35] epoch[7](4000/20000) Loss: 1.3151 Acc@1: 69.51 Acc@5: 87.82 time: 356.4738
[2021-11-10 10:20:31] epoch[7](6000/20000) Loss: 1.3148 Acc@1: 69.53 Acc@5: 87.83 time: 355.8594
[2021-11-10 10:26:26] epoch[7](8000/20000) Loss: 1.3139 Acc@1: 69.54 Acc@5: 87.87 time: 355.7704
[2021-11-10 10:32:23] epoch[7](10000/20000) Loss: 1.3125 Acc@1: 69.59 Acc@5: 87.88 time: 356.2529
[2021-11-10 10:38:19] epoch[7](12000/20000) Loss: 1.3130 Acc@1: 69.59 Acc@5: 87.87 time: 355.9078
[2021-11-10 10:44:14] epoch[7](14000/20000) Loss: 1.3128 Acc@1: 69.56 Acc@5: 87.87 time: 355.7919
[2021-11-10 10:50:10] epoch[7](16000/20000) Loss: 1.3131 Acc@1: 69.58 Acc@5: 87.88 time: 355.7708
[2021-11-10 10:56:06] epoch[7](18000/20000) Loss: 1.3136 Acc@1: 69.58 Acc@5: 87.87 time: 356.2857
[2021-11-10 11:02:02] epoch[7](20000/20000) Loss: 1.3132 Acc@1: 69.60 Acc@5: 87.89 time: 355.6885
[2021-11-10 11:02:06] train    Loss: 1.3132 Acc@1: 69.60 Acc@5: 87.89 time: 3566.0779
[2021-11-10 11:03:40] validate Loss: 1.1075 Acc@1: 72.40 Acc@5: 90.95 time: 94.4383
[2021-11-10 11:03:40] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-10 11:03:40] epoch 8 learning_rate 0.0001 
[2021-11-10 11:09:39] epoch[8](2000/20000) Loss: 1.3003 Acc@1: 69.88 Acc@5: 87.94 time: 358.6035
[2021-11-10 11:15:36] epoch[8](4000/20000) Loss: 1.3018 Acc@1: 69.88 Acc@5: 87.96 time: 356.6255
[2021-11-10 11:21:31] epoch[8](6000/20000) Loss: 1.3008 Acc@1: 69.89 Acc@5: 87.98 time: 355.8057
[2021-11-10 11:27:28] epoch[8](8000/20000) Loss: 1.3010 Acc@1: 69.90 Acc@5: 87.97 time: 356.1636
[2021-11-10 11:33:23] epoch[8](10000/20000) Loss: 1.3009 Acc@1: 69.90 Acc@5: 87.99 time: 355.8173
[2021-11-10 11:39:20] epoch[8](12000/20000) Loss: 1.3016 Acc@1: 69.90 Acc@5: 87.97 time: 356.2388
[2021-11-10 11:45:16] epoch[8](14000/20000) Loss: 1.3023 Acc@1: 69.89 Acc@5: 87.96 time: 356.0433
[2021-11-10 11:51:12] epoch[8](16000/20000) Loss: 1.3029 Acc@1: 69.86 Acc@5: 87.95 time: 355.8447
[2021-11-10 11:57:08] epoch[8](18000/20000) Loss: 1.3017 Acc@1: 69.88 Acc@5: 87.96 time: 355.8913
[2021-11-10 12:03:04] epoch[8](20000/20000) Loss: 1.3022 Acc@1: 69.86 Acc@5: 87.96 time: 356.3279
[2021-11-10 12:03:07] train    Loss: 1.3023 Acc@1: 69.86 Acc@5: 87.96 time: 3566.9974
[2021-11-10 12:04:43] validate Loss: 1.1033 Acc@1: 72.46 Acc@5: 90.97 time: 95.8519
[2021-11-10 12:04:44] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-10 12:04:44] epoch 9 learning_rate 0.0001 
[2021-11-10 12:10:42] epoch[9](2000/20000) Loss: 1.2926 Acc@1: 70.14 Acc@5: 88.10 time: 358.8134
[2021-11-10 12:16:38] epoch[9](4000/20000) Loss: 1.2890 Acc@1: 70.17 Acc@5: 88.17 time: 355.7133
[2021-11-10 12:22:34] epoch[9](6000/20000) Loss: 1.2869 Acc@1: 70.23 Acc@5: 88.20 time: 355.8401
[2021-11-10 12:28:30] epoch[9](8000/20000) Loss: 1.2891 Acc@1: 70.19 Acc@5: 88.17 time: 355.7487
[2021-11-10 12:34:26] epoch[9](10000/20000) Loss: 1.2897 Acc@1: 70.16 Acc@5: 88.16 time: 356.5820
[2021-11-10 12:40:22] epoch[9](12000/20000) Loss: 1.2903 Acc@1: 70.14 Acc@5: 88.14 time: 355.7181
[2021-11-10 12:46:18] epoch[9](14000/20000) Loss: 1.2906 Acc@1: 70.13 Acc@5: 88.13 time: 355.7334
[2021-11-10 12:52:13] epoch[9](16000/20000) Loss: 1.2913 Acc@1: 70.09 Acc@5: 88.13 time: 355.6582
[2021-11-10 12:58:09] epoch[9](18000/20000) Loss: 1.2929 Acc@1: 70.05 Acc@5: 88.11 time: 355.8916
[2021-11-10 13:04:06] epoch[9](20000/20000) Loss: 1.2930 Acc@1: 70.05 Acc@5: 88.11 time: 356.3539
[2021-11-10 13:04:09] train    Loss: 1.2930 Acc@1: 70.05 Acc@5: 88.11 time: 3565.6846
[2021-11-10 13:05:48] validate Loss: 1.0983 Acc@1: 72.50 Acc@5: 91.06 time: 98.2791
[2021-11-10 13:05:48] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-10 13:05:48] epoch 10 learning_rate 0.0001 
[2021-11-10 13:11:47] epoch[10](2000/20000) Loss: 1.2810 Acc@1: 70.24 Acc@5: 88.24 time: 358.6512
[2021-11-10 13:17:42] epoch[10](4000/20000) Loss: 1.2796 Acc@1: 70.31 Acc@5: 88.30 time: 355.5957
[2021-11-10 13:23:38] epoch[10](6000/20000) Loss: 1.2807 Acc@1: 70.26 Acc@5: 88.24 time: 355.6960
[2021-11-10 13:29:34] epoch[10](8000/20000) Loss: 1.2817 Acc@1: 70.28 Acc@5: 88.24 time: 355.7149
[2021-11-10 13:35:29] epoch[10](10000/20000) Loss: 1.2822 Acc@1: 70.30 Acc@5: 88.23 time: 355.6646
[2021-11-10 13:41:25] epoch[10](12000/20000) Loss: 1.2842 Acc@1: 70.27 Acc@5: 88.20 time: 355.6980
[2021-11-10 13:47:21] epoch[10](14000/20000) Loss: 1.2846 Acc@1: 70.26 Acc@5: 88.21 time: 356.2163
[2021-11-10 13:53:17] epoch[10](16000/20000) Loss: 1.2854 Acc@1: 70.24 Acc@5: 88.19 time: 355.4737
[2021-11-10 13:59:13] epoch[10](18000/20000) Loss: 1.2866 Acc@1: 70.20 Acc@5: 88.18 time: 355.8168
[2021-11-10 14:05:08] epoch[10](20000/20000) Loss: 1.2866 Acc@1: 70.20 Acc@5: 88.18 time: 355.6678
[2021-11-10 14:05:12] train    Loss: 1.2866 Acc@1: 70.21 Acc@5: 88.18 time: 3563.7766
[2021-11-10 14:06:48] validate Loss: 1.0949 Acc@1: 72.64 Acc@5: 91.07 time: 96.6733
[2021-11-10 14:06:49] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-10 14:06:49] epoch 11 learning_rate 0.0001 
[2021-11-10 14:12:47] epoch[11](2000/20000) Loss: 1.2709 Acc@1: 70.63 Acc@5: 88.37 time: 358.5777
[2021-11-10 14:18:43] epoch[11](4000/20000) Loss: 1.2739 Acc@1: 70.50 Acc@5: 88.34 time: 355.5966
[2021-11-10 14:24:39] epoch[11](6000/20000) Loss: 1.2773 Acc@1: 70.45 Acc@5: 88.28 time: 355.8699
[2021-11-10 14:30:36] epoch[11](8000/20000) Loss: 1.2777 Acc@1: 70.39 Acc@5: 88.29 time: 356.9782
[2021-11-10 14:36:33] epoch[11](10000/20000) Loss: 1.2789 Acc@1: 70.36 Acc@5: 88.27 time: 356.7887
[2021-11-10 14:42:30] epoch[11](12000/20000) Loss: 1.2792 Acc@1: 70.36 Acc@5: 88.27 time: 356.9299
[2021-11-10 14:48:27] epoch[11](14000/20000) Loss: 1.2800 Acc@1: 70.34 Acc@5: 88.27 time: 356.8790
[2021-11-10 14:54:23] epoch[11](16000/20000) Loss: 1.2808 Acc@1: 70.32 Acc@5: 88.25 time: 356.7417
[2021-11-10 15:00:20] epoch[11](18000/20000) Loss: 1.2814 Acc@1: 70.31 Acc@5: 88.24 time: 357.0232
[2021-11-10 15:06:18] epoch[11](20000/20000) Loss: 1.2821 Acc@1: 70.29 Acc@5: 88.23 time: 357.2272
[2021-11-10 15:06:21] train    Loss: 1.2822 Acc@1: 70.29 Acc@5: 88.23 time: 3572.2817
[2021-11-10 15:08:03] validate Loss: 1.0946 Acc@1: 72.76 Acc@5: 91.02 time: 102.1030
[2021-11-10 15:08:04] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-10 15:08:04] epoch 12 learning_rate 0.0001 
[2021-11-10 15:14:04] epoch[12](2000/20000) Loss: 1.2688 Acc@1: 70.65 Acc@5: 88.42 time: 360.3989
[2021-11-10 15:20:01] epoch[12](4000/20000) Loss: 1.2726 Acc@1: 70.53 Acc@5: 88.34 time: 357.4207
[2021-11-10 15:25:58] epoch[12](6000/20000) Loss: 1.2745 Acc@1: 70.47 Acc@5: 88.31 time: 356.7267
[2021-11-10 15:31:55] epoch[12](8000/20000) Loss: 1.2745 Acc@1: 70.47 Acc@5: 88.33 time: 356.7890
[2021-11-10 15:37:52] epoch[12](10000/20000) Loss: 1.2737 Acc@1: 70.52 Acc@5: 88.35 time: 356.6679
[2021-11-10 15:43:48] epoch[12](12000/20000) Loss: 1.2741 Acc@1: 70.50 Acc@5: 88.35 time: 356.7609
[2021-11-10 15:49:45] epoch[12](14000/20000) Loss: 1.2756 Acc@1: 70.47 Acc@5: 88.34 time: 356.8252
[2021-11-10 15:55:42] epoch[12](16000/20000) Loss: 1.2768 Acc@1: 70.45 Acc@5: 88.31 time: 357.1673
[2021-11-10 16:01:39] epoch[12](18000/20000) Loss: 1.2766 Acc@1: 70.45 Acc@5: 88.31 time: 357.0471
[2021-11-10 16:07:37] epoch[12](20000/20000) Loss: 1.2767 Acc@1: 70.45 Acc@5: 88.30 time: 357.1046
[2021-11-10 16:07:40] train    Loss: 1.2767 Acc@1: 70.45 Acc@5: 88.30 time: 3576.6572
[2021-11-10 16:09:28] validate Loss: 1.0947 Acc@1: 72.74 Acc@5: 91.12 time: 107.3642
[2021-11-10 16:09:28] epoch 13 learning_rate 0.0001 
[2021-11-10 16:15:28] epoch[13](2000/20000) Loss: 1.2653 Acc@1: 70.79 Acc@5: 88.47 time: 360.4352
[2021-11-10 16:21:25] epoch[13](4000/20000) Loss: 1.2704 Acc@1: 70.67 Acc@5: 88.38 time: 357.0139
[2021-11-10 16:27:22] epoch[13](6000/20000) Loss: 1.2710 Acc@1: 70.62 Acc@5: 88.36 time: 356.7896
[2021-11-10 16:33:19] epoch[13](8000/20000) Loss: 1.2699 Acc@1: 70.60 Acc@5: 88.39 time: 356.9493
[2021-11-10 16:39:16] epoch[13](10000/20000) Loss: 1.2722 Acc@1: 70.56 Acc@5: 88.38 time: 356.9648
[2021-11-10 16:45:12] epoch[13](12000/20000) Loss: 1.2729 Acc@1: 70.55 Acc@5: 88.37 time: 356.5875
[2021-11-10 16:51:09] epoch[13](14000/20000) Loss: 1.2730 Acc@1: 70.54 Acc@5: 88.36 time: 356.8897
[2021-11-10 16:57:06] epoch[13](16000/20000) Loss: 1.2742 Acc@1: 70.50 Acc@5: 88.36 time: 356.9729
[2021-11-10 17:03:04] epoch[13](18000/20000) Loss: 1.2738 Acc@1: 70.49 Acc@5: 88.37 time: 357.6117
[2021-11-10 17:09:01] epoch[13](20000/20000) Loss: 1.2746 Acc@1: 70.47 Acc@5: 88.35 time: 357.0346
[2021-11-10 17:09:05] train    Loss: 1.2746 Acc@1: 70.47 Acc@5: 88.35 time: 3576.8828
[2021-11-10 17:10:45] validate Loss: 1.0847 Acc@1: 72.93 Acc@5: 91.19 time: 100.4860
[2021-11-10 17:10:45] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-10 17:10:45] epoch 14 learning_rate 0.0001 
[2021-11-10 17:16:46] epoch[14](2000/20000) Loss: 1.2656 Acc@1: 70.65 Acc@5: 88.60 time: 360.2225
[2021-11-10 17:22:43] epoch[14](4000/20000) Loss: 1.2639 Acc@1: 70.71 Acc@5: 88.53 time: 356.9485
[2021-11-10 17:28:39] epoch[14](6000/20000) Loss: 1.2675 Acc@1: 70.63 Acc@5: 88.46 time: 356.7346
[2021-11-10 17:34:36] epoch[14](8000/20000) Loss: 1.2685 Acc@1: 70.63 Acc@5: 88.43 time: 356.9087
[2021-11-10 17:40:33] epoch[14](10000/20000) Loss: 1.2696 Acc@1: 70.63 Acc@5: 88.42 time: 356.9501
[2021-11-10 17:46:30] epoch[14](12000/20000) Loss: 1.2698 Acc@1: 70.60 Acc@5: 88.41 time: 357.0825
[2021-11-10 17:52:28] epoch[14](14000/20000) Loss: 1.2706 Acc@1: 70.57 Acc@5: 88.38 time: 357.5801
[2021-11-10 17:58:25] epoch[14](16000/20000) Loss: 1.2715 Acc@1: 70.55 Acc@5: 88.38 time: 356.8684
[2021-11-10 18:04:21] epoch[14](18000/20000) Loss: 1.2713 Acc@1: 70.56 Acc@5: 88.38 time: 356.6693
[2021-11-10 18:10:18] epoch[14](20000/20000) Loss: 1.2719 Acc@1: 70.54 Acc@5: 88.37 time: 356.8154
[2021-11-10 18:10:22] train    Loss: 1.2720 Acc@1: 70.54 Acc@5: 88.37 time: 3576.3686
[2021-11-10 18:12:03] validate Loss: 1.0887 Acc@1: 72.60 Acc@5: 91.22 time: 100.7206
[2021-11-10 18:12:03] epoch 15 learning_rate 0.0001 
[2021-11-10 18:18:03] epoch[15](2000/20000) Loss: 1.2671 Acc@1: 70.62 Acc@5: 88.47 time: 360.1522
[2021-11-10 18:24:00] epoch[15](4000/20000) Loss: 1.2616 Acc@1: 70.77 Acc@5: 88.53 time: 356.8511
[2021-11-10 18:29:57] epoch[15](6000/20000) Loss: 1.2641 Acc@1: 70.72 Acc@5: 88.50 time: 356.9968
[2021-11-10 18:35:54] epoch[15](8000/20000) Loss: 1.2650 Acc@1: 70.68 Acc@5: 88.47 time: 357.8492
[2021-11-10 18:41:51] epoch[15](10000/20000) Loss: 1.2653 Acc@1: 70.70 Acc@5: 88.48 time: 356.9553
[2021-11-10 18:47:47] epoch[15](12000/20000) Loss: 1.2654 Acc@1: 70.72 Acc@5: 88.48 time: 356.0413
[2021-11-10 18:53:43] epoch[15](14000/20000) Loss: 1.2655 Acc@1: 70.71 Acc@5: 88.47 time: 356.0592
[2021-11-10 18:59:40] epoch[15](16000/20000) Loss: 1.2662 Acc@1: 70.71 Acc@5: 88.45 time: 356.5638
[2021-11-10 19:05:36] epoch[15](18000/20000) Loss: 1.2673 Acc@1: 70.67 Acc@5: 88.43 time: 356.2409
[2021-11-10 19:11:33] epoch[15](20000/20000) Loss: 1.2673 Acc@1: 70.67 Acc@5: 88.45 time: 356.4828
[2021-11-10 19:11:36] train    Loss: 1.2673 Acc@1: 70.67 Acc@5: 88.44 time: 3573.7848
[2021-11-10 19:13:18] validate Loss: 1.0855 Acc@1: 72.76 Acc@5: 91.15 time: 101.6105
[2021-11-10 19:13:18] epoch 16 learning_rate 0.0001 
[2021-11-10 19:19:18] epoch[16](2000/20000) Loss: 1.2533 Acc@1: 71.02 Acc@5: 88.64 time: 359.5813
[2021-11-10 19:25:14] epoch[16](4000/20000) Loss: 1.2604 Acc@1: 70.87 Acc@5: 88.52 time: 356.6871
[2021-11-10 19:31:11] epoch[16](6000/20000) Loss: 1.2607 Acc@1: 70.89 Acc@5: 88.52 time: 356.7217
[2021-11-10 19:37:08] epoch[16](8000/20000) Loss: 1.2629 Acc@1: 70.84 Acc@5: 88.48 time: 356.8922
[2021-11-10 19:43:05] epoch[16](10000/20000) Loss: 1.2624 Acc@1: 70.81 Acc@5: 88.50 time: 356.8951
[2021-11-10 19:49:02] epoch[16](12000/20000) Loss: 1.2641 Acc@1: 70.77 Acc@5: 88.48 time: 357.0148
[2021-11-10 19:54:59] epoch[16](14000/20000) Loss: 1.2652 Acc@1: 70.73 Acc@5: 88.47 time: 356.9657
[2021-11-10 20:00:56] epoch[16](16000/20000) Loss: 1.2660 Acc@1: 70.69 Acc@5: 88.47 time: 357.2778
[2021-11-10 20:06:53] epoch[16](18000/20000) Loss: 1.2665 Acc@1: 70.68 Acc@5: 88.47 time: 357.1929
[2021-11-10 20:12:51] epoch[16](20000/20000) Loss: 1.2668 Acc@1: 70.66 Acc@5: 88.47 time: 357.8447
[2021-11-10 20:12:55] train    Loss: 1.2668 Acc@1: 70.66 Acc@5: 88.47 time: 3576.8550
[2021-11-10 20:14:45] validate Loss: 1.0832 Acc@1: 72.83 Acc@5: 91.16 time: 110.0661
[2021-11-10 20:14:45] epoch 17 learning_rate 0.0001 
[2021-11-10 20:20:46] epoch[17](2000/20000) Loss: 1.2581 Acc@1: 70.91 Acc@5: 88.65 time: 361.0279
[2021-11-10 20:26:43] epoch[17](4000/20000) Loss: 1.2558 Acc@1: 70.93 Acc@5: 88.65 time: 357.2090
[2021-11-10 20:32:40] epoch[17](6000/20000) Loss: 1.2589 Acc@1: 70.85 Acc@5: 88.59 time: 357.0330
[2021-11-10 20:38:37] epoch[17](8000/20000) Loss: 1.2607 Acc@1: 70.79 Acc@5: 88.55 time: 357.0189
[2021-11-10 20:44:35] epoch[17](10000/20000) Loss: 1.2613 Acc@1: 70.78 Acc@5: 88.54 time: 357.3099
[2021-11-10 20:50:32] epoch[17](12000/20000) Loss: 1.2623 Acc@1: 70.76 Acc@5: 88.52 time: 357.4737
[2021-11-10 20:56:29] epoch[17](14000/20000) Loss: 1.2626 Acc@1: 70.75 Acc@5: 88.52 time: 357.4805
[2021-11-10 21:02:28] epoch[17](16000/20000) Loss: 1.2625 Acc@1: 70.75 Acc@5: 88.52 time: 358.3185
[2021-11-10 21:08:25] epoch[17](18000/20000) Loss: 1.2631 Acc@1: 70.73 Acc@5: 88.51 time: 357.4049
[2021-11-10 21:14:22] epoch[17](20000/20000) Loss: 1.2628 Acc@1: 70.75 Acc@5: 88.51 time: 357.1439
[2021-11-10 21:14:26] train    Loss: 1.2629 Acc@1: 70.75 Acc@5: 88.51 time: 3581.1154
[2021-11-10 21:16:09] validate Loss: 1.0836 Acc@1: 72.85 Acc@5: 91.17 time: 102.4965
[2021-11-10 21:16:09] epoch 18 learning_rate 0.0001 
[2021-11-10 21:22:09] epoch[18](2000/20000) Loss: 1.2536 Acc@1: 70.98 Acc@5: 88.64 time: 360.6414
[2021-11-10 21:28:06] epoch[18](4000/20000) Loss: 1.2530 Acc@1: 71.04 Acc@5: 88.61 time: 357.0745
[2021-11-10 21:34:03] epoch[18](6000/20000) Loss: 1.2554 Acc@1: 70.99 Acc@5: 88.57 time: 357.2596
[2021-11-10 21:40:01] epoch[18](8000/20000) Loss: 1.2543 Acc@1: 71.02 Acc@5: 88.60 time: 357.4682
[2021-11-10 21:45:58] epoch[18](10000/20000) Loss: 1.2574 Acc@1: 70.93 Acc@5: 88.55 time: 357.4481
[2021-11-10 21:51:57] epoch[18](12000/20000) Loss: 1.2579 Acc@1: 70.90 Acc@5: 88.57 time: 358.5718
[2021-11-10 21:57:54] epoch[18](14000/20000) Loss: 1.2582 Acc@1: 70.89 Acc@5: 88.57 time: 356.9848
[2021-11-10 22:03:51] epoch[18](16000/20000) Loss: 1.2592 Acc@1: 70.86 Acc@5: 88.56 time: 357.2244
[2021-11-10 22:09:48] epoch[18](18000/20000) Loss: 1.2600 Acc@1: 70.84 Acc@5: 88.56 time: 357.0736
[2021-11-10 22:15:45] epoch[18](20000/20000) Loss: 1.2602 Acc@1: 70.84 Acc@5: 88.56 time: 357.0402
[2021-11-10 22:15:49] train    Loss: 1.2602 Acc@1: 70.84 Acc@5: 88.56 time: 3580.4265
[2021-11-10 22:17:27] validate Loss: 1.0822 Acc@1: 72.94 Acc@5: 91.19 time: 97.9396
[2021-11-10 22:17:27] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-10 22:17:27] epoch 19 learning_rate 0.0001 
[2021-11-10 22:23:27] epoch[19](2000/20000) Loss: 1.2603 Acc@1: 70.84 Acc@5: 88.58 time: 359.8764
[2021-11-10 22:29:24] epoch[19](4000/20000) Loss: 1.2577 Acc@1: 70.89 Acc@5: 88.60 time: 356.7456
[2021-11-10 22:35:20] epoch[19](6000/20000) Loss: 1.2594 Acc@1: 70.88 Acc@5: 88.59 time: 356.4912
[2021-11-10 22:41:18] epoch[19](8000/20000) Loss: 1.2577 Acc@1: 70.93 Acc@5: 88.58 time: 357.4487
[2021-11-10 22:47:15] epoch[19](10000/20000) Loss: 1.2578 Acc@1: 70.89 Acc@5: 88.59 time: 356.7606
[2021-11-10 22:53:11] epoch[19](12000/20000) Loss: 1.2578 Acc@1: 70.90 Acc@5: 88.59 time: 356.8991
[2021-11-10 22:59:08] epoch[19](14000/20000) Loss: 1.2579 Acc@1: 70.89 Acc@5: 88.58 time: 356.2463
[2021-11-10 23:05:04] epoch[19](16000/20000) Loss: 1.2583 Acc@1: 70.86 Acc@5: 88.59 time: 355.9434
[2021-11-10 23:10:59] epoch[19](18000/20000) Loss: 1.2584 Acc@1: 70.86 Acc@5: 88.59 time: 355.7155
[2021-11-10 23:16:55] epoch[19](20000/20000) Loss: 1.2586 Acc@1: 70.84 Acc@5: 88.59 time: 356.0249
[2021-11-10 23:16:59] train    Loss: 1.2586 Acc@1: 70.84 Acc@5: 88.59 time: 3571.6925
[2021-11-10 23:18:34] validate Loss: 1.0837 Acc@1: 72.81 Acc@5: 91.17 time: 95.5387
[2021-11-10 23:18:34] epoch 20 learning_rate 1e-05 
[2021-11-10 23:24:33] epoch[20](2000/20000) Loss: 1.2495 Acc@1: 71.09 Acc@5: 88.61 time: 358.7191
[2021-11-10 23:30:29] epoch[20](4000/20000) Loss: 1.2421 Acc@1: 71.23 Acc@5: 88.76 time: 356.1033
[2021-11-10 23:36:25] epoch[20](6000/20000) Loss: 1.2390 Acc@1: 71.28 Acc@5: 88.83 time: 356.0346
[2021-11-10 23:42:21] epoch[20](8000/20000) Loss: 1.2368 Acc@1: 71.32 Acc@5: 88.85 time: 355.6196
[2021-11-10 23:48:17] epoch[20](10000/20000) Loss: 1.2367 Acc@1: 71.35 Acc@5: 88.84 time: 355.7851
[2021-11-10 23:54:12] epoch[20](12000/20000) Loss: 1.2336 Acc@1: 71.41 Acc@5: 88.88 time: 355.7061
[2021-11-11 00:00:08] epoch[20](14000/20000) Loss: 1.2325 Acc@1: 71.44 Acc@5: 88.89 time: 355.7788
[2021-11-11 00:06:04] epoch[20](16000/20000) Loss: 1.2313 Acc@1: 71.47 Acc@5: 88.91 time: 355.7785
[2021-11-11 00:12:00] epoch[20](18000/20000) Loss: 1.2303 Acc@1: 71.50 Acc@5: 88.92 time: 355.6068
[2021-11-11 00:17:55] epoch[20](20000/20000) Loss: 1.2294 Acc@1: 71.52 Acc@5: 88.94 time: 355.7303
[2021-11-11 00:17:59] train    Loss: 1.2294 Acc@1: 71.52 Acc@5: 88.94 time: 3564.4334
[2021-11-11 00:19:35] validate Loss: 1.0668 Acc@1: 73.24 Acc@5: 91.45 time: 96.0010
[2021-11-11 00:19:35] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-11 00:19:35] epoch 21 learning_rate 1e-05 
[2021-11-11 00:25:35] epoch[21](2000/20000) Loss: 1.2287 Acc@1: 71.42 Acc@5: 88.97 time: 359.2312
[2021-11-11 00:31:30] epoch[21](4000/20000) Loss: 1.2244 Acc@1: 71.63 Acc@5: 89.01 time: 355.9390
[2021-11-11 00:37:26] epoch[21](6000/20000) Loss: 1.2235 Acc@1: 71.64 Acc@5: 89.02 time: 355.9742
[2021-11-11 00:43:22] epoch[21](8000/20000) Loss: 1.2243 Acc@1: 71.61 Acc@5: 89.00 time: 355.7748
[2021-11-11 00:49:18] epoch[21](10000/20000) Loss: 1.2238 Acc@1: 71.63 Acc@5: 89.00 time: 355.9167
[2021-11-11 00:55:14] epoch[21](12000/20000) Loss: 1.2235 Acc@1: 71.61 Acc@5: 89.01 time: 355.8644
[2021-11-11 01:01:10] epoch[21](14000/20000) Loss: 1.2231 Acc@1: 71.62 Acc@5: 89.02 time: 355.8339
[2021-11-11 01:07:06] epoch[21](16000/20000) Loss: 1.2228 Acc@1: 71.63 Acc@5: 89.02 time: 355.7760
[2021-11-11 01:13:02] epoch[21](18000/20000) Loss: 1.2223 Acc@1: 71.63 Acc@5: 89.02 time: 356.0095
[2021-11-11 01:18:57] epoch[21](20000/20000) Loss: 1.2224 Acc@1: 71.63 Acc@5: 89.02 time: 355.7583
[2021-11-11 01:19:01] train    Loss: 1.2224 Acc@1: 71.63 Acc@5: 89.02 time: 3565.6868
[2021-11-11 01:20:36] validate Loss: 1.0631 Acc@1: 73.40 Acc@5: 91.45 time: 94.6310
[2021-11-11 01:20:36] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-11 01:20:36] epoch 22 learning_rate 1e-05 
[2021-11-11 01:26:35] epoch[22](2000/20000) Loss: 1.2163 Acc@1: 71.85 Acc@5: 88.99 time: 358.8018
[2021-11-11 01:32:31] epoch[22](4000/20000) Loss: 1.2159 Acc@1: 71.83 Acc@5: 89.05 time: 355.7187
[2021-11-11 01:38:26] epoch[22](6000/20000) Loss: 1.2144 Acc@1: 71.89 Acc@5: 89.09 time: 355.8861
[2021-11-11 01:44:22] epoch[22](8000/20000) Loss: 1.2168 Acc@1: 71.82 Acc@5: 89.06 time: 355.8320
[2021-11-11 01:50:18] epoch[22](10000/20000) Loss: 1.2162 Acc@1: 71.83 Acc@5: 89.06 time: 355.4478
[2021-11-11 01:56:13] epoch[22](12000/20000) Loss: 1.2164 Acc@1: 71.83 Acc@5: 89.07 time: 355.5390
[2021-11-11 02:02:09] epoch[22](14000/20000) Loss: 1.2174 Acc@1: 71.80 Acc@5: 89.06 time: 355.5872
[2021-11-11 02:08:04] epoch[22](16000/20000) Loss: 1.2177 Acc@1: 71.80 Acc@5: 89.07 time: 355.3292
[2021-11-11 02:14:00] epoch[22](18000/20000) Loss: 1.2175 Acc@1: 71.80 Acc@5: 89.06 time: 355.4741
[2021-11-11 02:19:55] epoch[22](20000/20000) Loss: 1.2171 Acc@1: 71.79 Acc@5: 89.06 time: 355.5030
[2021-11-11 02:19:59] train    Loss: 1.2171 Acc@1: 71.79 Acc@5: 89.07 time: 3562.6117
[2021-11-11 02:21:23] validate Loss: 1.0629 Acc@1: 73.35 Acc@5: 91.48 time: 84.3379
[2021-11-11 02:21:23] epoch 23 learning_rate 1e-05 
[2021-11-11 02:27:21] epoch[23](2000/20000) Loss: 1.2145 Acc@1: 71.97 Acc@5: 89.01 time: 358.2656
[2021-11-11 02:33:16] epoch[23](4000/20000) Loss: 1.2141 Acc@1: 71.94 Acc@5: 89.06 time: 355.2278
[2021-11-11 02:39:12] epoch[23](6000/20000) Loss: 1.2133 Acc@1: 71.93 Acc@5: 89.11 time: 355.3216
[2021-11-11 02:45:07] epoch[23](8000/20000) Loss: 1.2139 Acc@1: 71.91 Acc@5: 89.13 time: 355.6544
[2021-11-11 02:51:03] epoch[23](10000/20000) Loss: 1.2132 Acc@1: 71.91 Acc@5: 89.15 time: 355.7318
[2021-11-11 02:56:59] epoch[23](12000/20000) Loss: 1.2138 Acc@1: 71.89 Acc@5: 89.14 time: 355.5846
[2021-11-11 03:02:54] epoch[23](14000/20000) Loss: 1.2131 Acc@1: 71.90 Acc@5: 89.15 time: 355.6213
[2021-11-11 03:08:50] epoch[23](16000/20000) Loss: 1.2140 Acc@1: 71.89 Acc@5: 89.12 time: 355.3384
[2021-11-11 03:14:45] epoch[23](18000/20000) Loss: 1.2147 Acc@1: 71.86 Acc@5: 89.10 time: 355.3759
[2021-11-11 03:20:41] epoch[23](20000/20000) Loss: 1.2156 Acc@1: 71.83 Acc@5: 89.09 time: 355.6554
[2021-11-11 03:20:44] train    Loss: 1.2156 Acc@1: 71.83 Acc@5: 89.09 time: 3561.2842
[2021-11-11 03:22:10] validate Loss: 1.0589 Acc@1: 73.43 Acc@5: 91.51 time: 85.3063
[2021-11-11 03:22:10] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-11 03:22:10] epoch 24 learning_rate 1e-05 
[2021-11-11 03:28:08] epoch[24](2000/20000) Loss: 1.2183 Acc@1: 71.87 Acc@5: 89.02 time: 358.4167
[2021-11-11 03:34:04] epoch[24](4000/20000) Loss: 1.2183 Acc@1: 71.77 Acc@5: 89.00 time: 355.7022
[2021-11-11 03:40:00] epoch[24](6000/20000) Loss: 1.2190 Acc@1: 71.75 Acc@5: 89.01 time: 355.8617
[2021-11-11 03:45:56] epoch[24](8000/20000) Loss: 1.2181 Acc@1: 71.75 Acc@5: 89.04 time: 356.0423
[2021-11-11 03:51:52] epoch[24](10000/20000) Loss: 1.2176 Acc@1: 71.77 Acc@5: 89.07 time: 355.8388
[2021-11-11 03:57:48] epoch[24](12000/20000) Loss: 1.2161 Acc@1: 71.78 Acc@5: 89.10 time: 355.8659
[2021-11-11 04:03:44] epoch[24](14000/20000) Loss: 1.2163 Acc@1: 71.79 Acc@5: 89.10 time: 355.8987
[2021-11-11 04:09:39] epoch[24](16000/20000) Loss: 1.2162 Acc@1: 71.77 Acc@5: 89.10 time: 355.6289
[2021-11-11 04:15:35] epoch[24](18000/20000) Loss: 1.2153 Acc@1: 71.78 Acc@5: 89.11 time: 355.7469
[2021-11-11 04:21:31] epoch[24](20000/20000) Loss: 1.2139 Acc@1: 71.81 Acc@5: 89.12 time: 355.8058
[2021-11-11 04:21:34] train    Loss: 1.2139 Acc@1: 71.81 Acc@5: 89.12 time: 3564.3367
[2021-11-11 04:23:02] validate Loss: 1.0594 Acc@1: 73.45 Acc@5: 91.45 time: 87.3719
[2021-11-11 04:23:02] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-11 04:23:02] epoch 25 learning_rate 1e-05 
[2021-11-11 04:29:01] epoch[25](2000/20000) Loss: 1.2070 Acc@1: 71.93 Acc@5: 89.23 time: 358.5620
[2021-11-11 04:34:57] epoch[25](4000/20000) Loss: 1.2112 Acc@1: 71.91 Acc@5: 89.14 time: 355.9684
[2021-11-11 04:40:52] epoch[25](6000/20000) Loss: 1.2139 Acc@1: 71.86 Acc@5: 89.09 time: 355.4867
[2021-11-11 04:46:48] epoch[25](8000/20000) Loss: 1.2122 Acc@1: 71.90 Acc@5: 89.13 time: 356.0508
[2021-11-11 04:52:44] epoch[25](10000/20000) Loss: 1.2117 Acc@1: 71.91 Acc@5: 89.15 time: 355.7129
[2021-11-11 04:58:40] epoch[25](12000/20000) Loss: 1.2105 Acc@1: 71.94 Acc@5: 89.16 time: 355.7388
[2021-11-11 05:04:35] epoch[25](14000/20000) Loss: 1.2108 Acc@1: 71.92 Acc@5: 89.16 time: 355.6685
[2021-11-11 05:10:31] epoch[25](16000/20000) Loss: 1.2112 Acc@1: 71.92 Acc@5: 89.16 time: 355.6125
[2021-11-11 05:16:26] epoch[25](18000/20000) Loss: 1.2117 Acc@1: 71.92 Acc@5: 89.15 time: 355.4665
[2021-11-11 05:22:22] epoch[25](20000/20000) Loss: 1.2124 Acc@1: 71.91 Acc@5: 89.13 time: 355.7668
[2021-11-11 05:22:26] train    Loss: 1.2124 Acc@1: 71.91 Acc@5: 89.13 time: 3563.5651
[2021-11-11 05:23:53] validate Loss: 1.0583 Acc@1: 73.46 Acc@5: 91.54 time: 87.0550
[2021-11-11 05:23:53] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-11 05:23:53] epoch 26 learning_rate 1e-05 
[2021-11-11 05:29:52] epoch[26](2000/20000) Loss: 1.2162 Acc@1: 71.88 Acc@5: 89.12 time: 359.1480
[2021-11-11 05:35:48] epoch[26](4000/20000) Loss: 1.2097 Acc@1: 71.93 Acc@5: 89.20 time: 355.8562
[2021-11-11 05:41:44] epoch[26](6000/20000) Loss: 1.2109 Acc@1: 71.89 Acc@5: 89.19 time: 355.7816
[2021-11-11 05:47:39] epoch[26](8000/20000) Loss: 1.2103 Acc@1: 71.90 Acc@5: 89.19 time: 355.7204
[2021-11-11 05:53:35] epoch[26](10000/20000) Loss: 1.2119 Acc@1: 71.89 Acc@5: 89.15 time: 355.6765
[2021-11-11 05:59:31] epoch[26](12000/20000) Loss: 1.2109 Acc@1: 71.91 Acc@5: 89.17 time: 355.7483
[2021-11-11 06:05:27] epoch[26](14000/20000) Loss: 1.2117 Acc@1: 71.90 Acc@5: 89.16 time: 355.8749
[2021-11-11 06:11:23] epoch[26](16000/20000) Loss: 1.2117 Acc@1: 71.90 Acc@5: 89.16 time: 355.9250
[2021-11-11 06:17:19] epoch[26](18000/20000) Loss: 1.2114 Acc@1: 71.90 Acc@5: 89.17 time: 355.9499
[2021-11-11 06:23:15] epoch[26](20000/20000) Loss: 1.2119 Acc@1: 71.88 Acc@5: 89.17 time: 355.9405
[2021-11-11 06:23:18] train    Loss: 1.2118 Acc@1: 71.88 Acc@5: 89.17 time: 3565.1759
[2021-11-11 06:24:45] validate Loss: 1.0550 Acc@1: 73.49 Acc@5: 91.62 time: 86.4039
[2021-11-11 06:24:45] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-11 06:24:45] epoch 27 learning_rate 1e-05 
[2021-11-11 06:30:43] epoch[27](2000/20000) Loss: 1.2072 Acc@1: 71.99 Acc@5: 89.23 time: 358.1897
[2021-11-11 06:36:39] epoch[27](4000/20000) Loss: 1.2069 Acc@1: 71.95 Acc@5: 89.24 time: 355.4362
[2021-11-11 06:42:34] epoch[27](6000/20000) Loss: 1.2089 Acc@1: 71.95 Acc@5: 89.19 time: 355.4060
[2021-11-11 06:48:30] epoch[27](8000/20000) Loss: 1.2087 Acc@1: 71.98 Acc@5: 89.19 time: 355.5874
[2021-11-11 06:54:25] epoch[27](10000/20000) Loss: 1.2093 Acc@1: 71.97 Acc@5: 89.17 time: 355.5292
[2021-11-11 07:00:21] epoch[27](12000/20000) Loss: 1.2096 Acc@1: 71.98 Acc@5: 89.16 time: 355.5514
[2021-11-11 07:06:16] epoch[27](14000/20000) Loss: 1.2103 Acc@1: 71.95 Acc@5: 89.17 time: 355.5196
[2021-11-11 07:12:12] epoch[27](16000/20000) Loss: 1.2104 Acc@1: 71.94 Acc@5: 89.15 time: 355.5714
[2021-11-11 07:18:07] epoch[27](18000/20000) Loss: 1.2113 Acc@1: 71.91 Acc@5: 89.14 time: 355.1745
[2021-11-11 07:24:03] epoch[27](20000/20000) Loss: 1.2117 Acc@1: 71.92 Acc@5: 89.13 time: 355.5941
[2021-11-11 07:24:06] train    Loss: 1.2118 Acc@1: 71.92 Acc@5: 89.13 time: 3561.0854
[2021-11-11 07:25:31] validate Loss: 1.0580 Acc@1: 73.51 Acc@5: 91.55 time: 84.6928
[2021-11-11 07:25:31] storing checkpoint:/pruned_checkpoint/resnet_50.pt
[2021-11-11 07:25:31] epoch 28 learning_rate 1e-05 
[2021-11-11 07:31:29] epoch[28](2000/20000) Loss: 1.2047 Acc@1: 72.14 Acc@5: 89.25 time: 358.0543
[2021-11-11 07:37:25] epoch[28](4000/20000) Loss: 1.2094 Acc@1: 72.04 Acc@5: 89.17 time: 355.5856
[2021-11-11 07:43:20] epoch[28](6000/20000) Loss: 1.2083 Acc@1: 72.03 Acc@5: 89.17 time: 355.5494
[2021-11-11 07:49:16] epoch[28](8000/20000) Loss: 1.2081 Acc@1: 71.98 Acc@5: 89.19 time: 355.5401
[2021-11-11 07:55:11] epoch[28](10000/20000) Loss: 1.2103 Acc@1: 71.95 Acc@5: 89.15 time: 355.5453
[2021-11-11 08:01:07] epoch[28](12000/20000) Loss: 1.2103 Acc@1: 71.95 Acc@5: 89.16 time: 355.4792
[2021-11-11 08:07:03] epoch[28](14000/20000) Loss: 1.2119 Acc@1: 71.92 Acc@5: 89.15 time: 355.6453
[2021-11-11 08:12:58] epoch[28](16000/20000) Loss: 1.2105 Acc@1: 71.95 Acc@5: 89.17 time: 355.4350
[2021-11-11 08:18:53] epoch[28](18000/20000) Loss: 1.2105 Acc@1: 71.94 Acc@5: 89.16 time: 355.4062
[2021-11-11 08:24:49] epoch[28](20000/20000) Loss: 1.2102 Acc@1: 71.94 Acc@5: 89.16 time: 355.5340
[2021-11-11 08:24:52] train    Loss: 1.2102 Acc@1: 71.94 Acc@5: 89.16 time: 3561.2661
[2021-11-11 08:26:18] validate Loss: 1.0565 Acc@1: 73.50 Acc@5: 91.59 time: 85.7418
[2021-11-11 08:26:18] epoch 29 learning_rate 1e-05 
[2021-11-11 08:32:17] epoch[29](2000/20000) Loss: 1.2080 Acc@1: 71.95 Acc@5: 89.27 time: 358.5300
[2021-11-11 08:38:12] epoch[29](4000/20000) Loss: 1.2103 Acc@1: 71.90 Acc@5: 89.18 time: 355.7290
[2021-11-11 08:44:08] epoch[29](6000/20000) Loss: 1.2074 Acc@1: 71.97 Acc@5: 89.24 time: 355.8490
[2021-11-11 08:50:05] epoch[29](8000/20000) Loss: 1.2083 Acc@1: 71.96 Acc@5: 89.21 time: 356.3334
[2021-11-11 08:56:00] epoch[29](10000/20000) Loss: 1.2076 Acc@1: 71.98 Acc@5: 89.22 time: 355.7802
[2021-11-11 09:01:56] epoch[29](12000/20000) Loss: 1.2079 Acc@1: 72.01 Acc@5: 89.21 time: 355.8172
[2021-11-11 09:07:52] epoch[29](14000/20000) Loss: 1.2076 Acc@1: 72.00 Acc@5: 89.23 time: 355.5667
[2021-11-11 09:13:48] epoch[29](16000/20000) Loss: 1.2080 Acc@1: 71.99 Acc@5: 89.21 time: 355.8182
[2021-11-11 09:19:44] epoch[29](18000/20000) Loss: 1.2080 Acc@1: 71.99 Acc@5: 89.22 time: 355.9383
[2021-11-11 09:25:39] epoch[29](20000/20000) Loss: 1.2085 Acc@1: 71.97 Acc@5: 89.21 time: 355.8609
[2021-11-11 09:25:43] train    Loss: 1.2084 Acc@1: 71.97 Acc@5: 89.21 time: 3564.7594
[2021-11-11 09:27:09] validate Loss: 1.0556 Acc@1: 73.44 Acc@5: 91.57 time: 86.4876
[2021-11-11 09:29:18] ResNet50(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): ModuleList(
    (0): Bottleneck(
      (conv1): Conv2d(64, 57, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(57, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(57, 153, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(153, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 153, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(153, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(153, 19, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(19, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(19, 153, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(153, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(153, 19, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(19, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(19, 153, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(153, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (layer2): ModuleList(
    (0): Bottleneck(
      (conv1): Conv2d(153, 102, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(102, 102, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(102, 307, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(153, 307, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(307, 38, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(38, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(38, 307, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(307, 38, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(38, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(38, 307, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(307, 38, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(38, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(38, 307, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(307, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (layer3): ModuleList(
    (0): Bottleneck(
      (conv1): Conv2d(307, 204, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(204, 716, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(716, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(307, 716, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(716, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(716, 76, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(76, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(76, 716, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(716, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(716, 76, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(76, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(76, 716, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(716, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(716, 76, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(76, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(76, 716, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(716, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(716, 76, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(76, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(76, 716, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(716, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(716, 76, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(76, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(76, 716, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(716, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (layer4): ModuleList(
    (0): Bottleneck(
      (conv1): Conv2d(716, 460, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(460, 460, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(460, 1843, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1843, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(716, 1843, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1843, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1843, 409, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(409, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(409, 358, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(358, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(358, 1843, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1843, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1843, 409, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(409, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(409, 358, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(358, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv3): Conv2d(358, 1843, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1843, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu3): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=1843, out_features=1000, bias=True)
)
[2021-11-11 09:29:18] storing pruned_model:/finally_pruned_model/resnet_50_111_fs.pt
[2021-11-11 09:30:34] validate Loss: 1.0556 Acc@1: 73.43 Acc@5: 91.57 time: 75.6701
[2021-11-11 09:30:34] finally model  Acc@1: 73.43 Acc@5: 91.57 flops: 1504688024.0 params:13728139.0
